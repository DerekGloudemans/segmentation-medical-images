{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Select Final Model",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerekGloudemans/segmentation-medical-images/blob/master/Select_Final_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TCkBvB9NxZP",
        "colab_type": "code",
        "outputId": "2e74d38a-45e5-479d-b65f-e42881eab1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wltslcNiN4yq",
        "colab_type": "code",
        "outputId": "0d5ede65-7bf1-4807-e2c4-1663e2fb09f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#%%capture \n",
        "!pip install -q --upgrade ipython==5.5.0\n",
        "!pip install -q --upgrade ipykernel==4.6.0\n",
        "!pip3 install torchvision\n",
        "!pip3 install opencv-python\n",
        "\n",
        "import ipywidgets\n",
        "import traitlets\n",
        "# imports\n",
        "\n",
        "# this seems to be a popular thing to do so I've done it here\n",
        "#from __future__ import print_function, division\n",
        "\n",
        "\n",
        "# torch and specific torch packages for convenience\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils import data\n",
        "from torch import multiprocessing\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# for convenient data loading, image representation and dataset management\n",
        "from torchvision import models, transforms\n",
        "import torchvision.transforms.functional as FT\n",
        "from PIL import Image, ImageFile, ImageStat\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from scipy.ndimage import affine_transform\n",
        "import cv2\n",
        "\n",
        "# always good to have\n",
        "import time\n",
        "import os\n",
        "import numpy as np    \n",
        "import _pickle as pickle\n",
        "import random\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import nibabel as nib\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▏                            | 10kB 29.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HSeCWeeN6xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        n_classes=1,\n",
        "        depth=3,\n",
        "        wf=4,\n",
        "        padding=True,\n",
        "        batch_norm=False,\n",
        "        up_mode='upconv',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(\n",
        "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "        innum = 1000\n",
        "        midnum = 100\n",
        "        outnum = 4*n_classes\n",
        "        self.reg = nn.Sequential(\n",
        "            nn.BatchNorm1d(innum),\n",
        "            nn.Linear(innum,midnum),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(midnum,outnum),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #for param in self.parameters():\n",
        "        #  param.requires_grad = True\n",
        "\n",
        "    def forward(self, x,BBOX = False):\n",
        "        blocks = []\n",
        "        \n",
        "        # encoder\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path) - 1:\n",
        "                blocks.append(x)\n",
        "                x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # do bbox regression here\n",
        "        if BBOX:\n",
        "          x_reg = x.view(-1)\n",
        "          x_reg = self.reg(x_reg)\n",
        "          bboxes = x_reg.view(4,-1)\n",
        "\n",
        "        # decoder\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "\n",
        "        #CHANGE THIS LINE FOR MULTIPLE OUTPUT CHANNELS\n",
        "        # apply per_class last layer and per-class Softmax \n",
        "        #x = nn.Softmax2d(self.last(x)) \n",
        "        x = torch.sigmoid(self.last(x))\n",
        "        \n",
        "        if BBOX:\n",
        "          return x, bboxes\n",
        "        else:\n",
        "          return x\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
        "            )\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[\n",
        "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Nifti_Dataset(data.Dataset):\n",
        "  def __init__(self,mode = \"view\",dim = 2,class_id = 1):\n",
        "    \"\"\"\n",
        "    Save the last 0.15 proportion of files after sorting for use as validation set.\n",
        "    Loads each slice of the input images as a separate image\n",
        "    mode - view,train or val, defined in same dataset to maintain data separation\n",
        "      view - performs transforms but does not normalize images\n",
        "      train - normalizes data and performs transforms\n",
        "      val - normalizes data, no augmenting transforms\n",
        "    dim - specifies dimension along which to slice image\n",
        "    \"\"\"\n",
        "\n",
        "    self.mode = mode\n",
        "    self.dim = dim\n",
        "    self.class_id = class_id\n",
        "\n",
        "    data_dir = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Training/img\"\n",
        "    label_dir = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Training/label\"\n",
        "\n",
        "    # get all data and label file names\n",
        "    self.data_files = []\n",
        "    for file in os.listdir(data_dir):\n",
        "      self.data_files.append(os.path.join(data_dir,file))\n",
        "    self.data_files.sort()\n",
        "\n",
        "    self.label_files = []\n",
        "    for file in os.listdir(label_dir):\n",
        "      self.label_files.append(os.path.join(label_dir,file))\n",
        "    self.label_files.sort()\n",
        "\n",
        "    # for each data_file\n",
        "    self.train_data = []\n",
        "    self.val_data = []\n",
        "\n",
        "    for i in range(len(self.data_files)):\n",
        "      data = nib.load(self.data_files[i])\n",
        "      data = np.array(data.get_fdata())\n",
        "\n",
        "      label = nib.load(self.label_files[i])\n",
        "      label = np.array(label.get_fdata()).astype(float)\n",
        "\n",
        "      identifier = self.data_files[i].split(\"_\")[0]\n",
        "      for slice in range(0,data.shape[dim]):\n",
        "\n",
        "        # get slices\n",
        "        if dim == 0:\n",
        "          data_slice = data[slice,:,:]\n",
        "          label_slice = label[slice,:,:]\n",
        "        elif dim == 1:\n",
        "          data_slice = data[:,slice,:]\n",
        "          label_slice = label[:,slice,:]\n",
        "        elif dim == 2:\n",
        "          data_slice = data[:,:,slice]\n",
        "          label_slice = label[:,:,slice]\n",
        "\n",
        "        mean,std = np.mean(data_slice),np.std(data_slice)\n",
        "        # define item dict to store info\n",
        "        item = {\n",
        "            \"identifier\":identifier,\n",
        "            \"slice\":slice,\n",
        "            \"data\":data_slice,\n",
        "            \"label\":label_slice,\n",
        "            \"mean\":mean,\n",
        "            \"std\":std\n",
        "            }\n",
        "\n",
        "        # check to make sure this example actually has organs in it\n",
        "        test = np.bincount(label_slice.astype(int).reshape(-1))\n",
        "        if len(test) == 1: \n",
        "          continue\n",
        "\n",
        "        # make sure 50% of examples have organ of interest in them\n",
        "        if class_id not in np.unique(label_slice.astype(int).reshape(-1)):\n",
        "            if np.random.rand() > 0.5:\n",
        "              continue\n",
        "\n",
        "        # assign to either training or validation data\n",
        "        if i < len(self.data_files) * 0.85:\n",
        "          self.train_data.append(item)\n",
        "        else:\n",
        "          self.val_data.append(item)\n",
        "\n",
        "      #break # to shorten loading time\n",
        "\n",
        "    # define some transforms for training dataset\n",
        "    self.train_transforms = transforms.Compose([\n",
        "          transforms.ColorJitter(brightness = 0.2,contrast = 0.2,saturation = 0.1),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.RandomErasing(p=0.0015, scale=(0.4, 0.6), ratio=(0.3, 3.3), value=0, inplace=False), # big\n",
        "          transforms.RandomErasing(p=0.003, scale=(0.1, 0.3), ratio=(0.3, 3.3), value=0, inplace=False), # medium\n",
        "          transforms.RandomErasing(p=0.004, scale=(0.05, 0.15), ratio=(0.3, 3.3), value=0, inplace=False),# small\n",
        "          transforms.RandomErasing(p=0.0035, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=0, inplace=False) # small\n",
        "          \n",
        "\n",
        "        ])\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      #get relevant dictionary from self memory\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        item = self.train_data[index]\n",
        "      else:\n",
        "        item = self.val_data[index]\n",
        "\n",
        "      x = Image.fromarray(item['data']).copy()\n",
        "      y = Image.fromarray(item['label']).copy()\n",
        "\n",
        "      # to grayscale\n",
        "      x  = FT.to_grayscale(x)\n",
        "      y = FT.to_grayscale(y)\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        # randomly flip and rotate both\n",
        "        FLIP = 0 #np.random.rand()\n",
        "        if FLIP > 0.5:\n",
        "          x = FT.hflip(x)\n",
        "          y = FT.hflip(y)\n",
        "\n",
        "        ROTATE = 0 #np.random.rand()*60 - 30\n",
        "        x  = x.rotate(ROTATE)\n",
        "        y = y.rotate(ROTATE,Image.NEAREST)\n",
        "\n",
        "      # resize to 224 on shorter dimension\n",
        "      x = FT.resize(x, 256)\n",
        "      y = FT.resize(y,256,Image.NEAREST)\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        # randomly jitter color of data and randomly erase data\n",
        "        x = self.train_transforms(x)\n",
        "      # to tensor\n",
        "      try:\n",
        "        x = FT.to_tensor(x)\n",
        "      except:\n",
        "        pass\n",
        "      y = FT.to_tensor(y)\n",
        "            \n",
        "      # normalize and repeat along color dimension if in train or val mode\n",
        "      if self.mode in ['train','val']:\n",
        "        #x = FT.normalize(x,[item['mean']],[item['std']])\n",
        "        #x = x.repeat(3,1,1)\n",
        "        pass\n",
        "\n",
        "      return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.mode in [\"train\",\"view\"]:\n",
        "      return len(self.train_data)\n",
        "    else:\n",
        "      return len(self.val_data)\n",
        "\n",
        "  def show(self,index):\n",
        "    data,label = self[index]\n",
        "    plt.figure()\n",
        "    plt.subplot(121)\n",
        "    data = data.detach()\n",
        "    plt.imshow(data[0],cmap = \"gray\")\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(label[0],cmap = \"gray\")\n",
        "    plt.show()\n",
        "    # convert each tensor to numpy array\n",
        "\n",
        "  def show_slices(self,idx = 0,dim = 0,organ_id = None):\n",
        "    \"\"\"\n",
        "    A nice utility function for plotting all of the slices along a given dimension\n",
        "    idx - indexes all NIfTI images in dataset\n",
        "    dim - indexes dimension of image\n",
        "    organ_id - if not None, all other organs removed from label\n",
        "    \"\"\"\n",
        "    data = nib.load(self.data_files[idx])\n",
        "    label = nib.load(self.label_files[idx])\n",
        "\n",
        "    data = data.get_fdata()\n",
        "    data = np.array(data)\n",
        "    label = label.get_fdata()\n",
        "    label = np.array(label)\n",
        "\n",
        "    for slice in range(0,data.shape[dim]):\n",
        "      if dim == 0:\n",
        "            data_slice = data[slice,:,:]\n",
        "            label_slice = label[slice,:,:]\n",
        "      elif dim == 1:\n",
        "        data_slice = data[:,slice,:]\n",
        "        label_slice = label[:,slice,:]\n",
        "      elif dim == 2:\n",
        "        data_slice = data[:,:,slice]\n",
        "        label_slice = label[:,:,slice]\n",
        "\n",
        "      if organ_id is not None:\n",
        "        # if a specific label is to be looked at, 0 all others\n",
        "        label_slice = 1.0 - np.ceil(np.abs(label_slice.astype(float)-organ_id)/15.0)\n",
        "\n",
        "      print(np.unique(label_slice))\n",
        "      plt.figure()\n",
        "      plt.subplot(121)\n",
        "      plt.imshow(data_slice,cmap = \"gray\")\n",
        "\n",
        "      plt.subplot(122)\n",
        "      plt.imshow(label_slice,cmap = \"gray\")\n",
        "      plt.show()\n",
        "\n",
        "  def len_3d(self):\n",
        "    return len(self.data_files)\n",
        "\n",
        "  def get_3d_array(self,idx):\n",
        "      \"\"\"\n",
        "      Loads a 3D image as a tensor as well as its label, mode, and file name\n",
        "      \"\"\"\n",
        "      assert idx < len(self.data_files) , \"3D image index out of range, there are {} 3D images\".format(len(self.data_files))\n",
        "\n",
        "      # load data and label as tensors\n",
        "      data = nib.load(self.data_files[idx])\n",
        "      label = nib.load(self.label_files[idx])\n",
        "\n",
        "      data = data.get_fdata()\n",
        "      data = torch.from_numpy(np.array(data))\n",
        "      label = label.get_fdata()\n",
        "      label = torch.from_numpy(np.array(label)).int()\n",
        "\n",
        "      # note whether image is training or validation set\n",
        "      if idx < 0.85 * len(self.data_files):\n",
        "        mode = \"train\"\n",
        "      else:\n",
        "        mode = \"val\"\n",
        "      \n",
        "      return data,label,mode,self.data_files[idx]\n",
        "\n",
        "def load_model(checkpoint_file,model,optimizer):\n",
        "  \"\"\"\n",
        "  Reloads a checkpoint, loading the model and optimizer state_dicts and \n",
        "  setting the start epoch\n",
        "  \"\"\"\n",
        "  checkpoint = torch.load(checkpoint_file)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  all_losses = checkpoint['losses']\n",
        "  all_accs = checkpoint['accs']\n",
        "\n",
        "  return model,optimizer,epoch,all_losses,all_accs\n",
        "\n",
        "\n",
        "def dt_refine(orig,tree):\n",
        "  \"\"\"\n",
        "  Applies decision tree ensembling to original outputs\n",
        "  orig = classes x l x w x h\n",
        "  \"\"\"\n",
        "  output = torch.zeros((orig.shape[1],orig.shape[2],orig.shape[3]))\n",
        "\n",
        "  for i in range(0,orig.shape[1]):\n",
        "    for j in range(0,orig.shape[2]):\n",
        "      \n",
        "        inp = np.zeros([orig.shape[3],17])\n",
        "        inp[:,:14] = orig[:,i,j,:].data.numpy().transpose()\n",
        "        inp[:,14] = i\n",
        "        inp[:,15] = j\n",
        "        for k in range(0,orig.shape[3]):\n",
        "          inp[k,16] = k\n",
        "\n",
        "        output[i,j,:] = torch.from_numpy(tree.predict(inp))\n",
        "  \n",
        "  return output\n",
        "\n",
        "\n",
        "def predict_by_slices(model,data,device,outfile = None):\n",
        "  \"\"\"\n",
        "  Takes in 3D tensor, slices and segments using UNET, and returns result\n",
        "  Note that result will be a [0,1] tensor corresponding to a certain class (whatever model class was used)\n",
        "  if outfile is not none, saves results in  that file \n",
        "  \"\"\"\n",
        "\n",
        "  num_slices = data.shape[2]\n",
        "  result = torch.zeros(data.shape)\n",
        "\n",
        "  for idx in range(num_slices):\n",
        "    # resize to 256 x 256\n",
        "    slice = data[:,:,idx]\n",
        "    original_shape = slice.shape\n",
        "\n",
        "    slice =  Image.fromarray(slice.data.numpy()).copy()\n",
        "    slice = FT.to_grayscale(slice)\n",
        "    slice = FT.to_tensor(slice)\n",
        "    slice = slice.unsqueeze(0)\n",
        "\n",
        "    slice = F.interpolate(slice,size = [256,256],mode = 'bilinear')\n",
        "\n",
        "    x = slice.to(device).float()\n",
        "\n",
        "    out_slice = model(x)\n",
        "    out_slice = F.interpolate(out_slice,original_shape,mode = 'nearest')\n",
        "    result[:,:,idx] = out_slice.data.cpu()\n",
        "\n",
        "  if outfile:\n",
        "    torch.save(result, outfile)\n",
        "\n",
        "  return result\n",
        "\n",
        "def segment(model,checkpoint_dict,data,device,outfile = None):\n",
        "  \"\"\"\n",
        "  Takes a 3D tensor and segments it with a series of models, predicting maximum class for each\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  result = torch.zeros((14,data.shape[0],data.shape[1],data.shape[2]))\n",
        "\n",
        "  for i in range(0,14):\n",
        "    checkpoint = torch.load(checkpoint_dict[i])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    result[i,:,:,:] = predict_by_slices(model,data,device,outfile = None)\n",
        "    #print(\"Finished class {}\".format(i))\n",
        "  \n",
        "  #result = result.round()\n",
        "  #result[0,:,:,:] = result[0,:,:,:].round()*0.3\n",
        "  #result = torch.argmax(result,dim = 0)\n",
        "\n",
        "  return result\n",
        "\n",
        "def Dice_3D(output,target,eps = 1e-07,threshold = 0.5):\n",
        "  \"\"\"\n",
        "  Assumes output is 4D and target is 3D\n",
        "  \"\"\"\n",
        "  per_class_dice = torch.zeros(14)\n",
        "  per_class_counts = torch.zeros(14)\n",
        "  ones = torch.ones(target[0].shape)\n",
        "  zeros = torch.zeros(target[0].shape)\n",
        "\n",
        "  for idx in range(0,14):\n",
        "    true_pos = torch.where(target == idx,ones,zeros)\n",
        "    pred_pos = torch.where(output[idx,:,:,:] > threshold,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos)\n",
        "    denominator = true_pos + pred_pos\n",
        "\n",
        "    per_class_dice[idx] = (numerator.sum()+eps)/(denominator.sum()+eps)\n",
        "    per_class_counts[idx] = true_pos.sum()\n",
        "  \n",
        "  # ignore 0s\n",
        "  total_dice = torch.sum(torch.mul(per_class_dice[1:].float(),per_class_counts[1:]) / torch.numel(target))\n",
        "  return per_class_dice,total_dice\n",
        "\n",
        "def Dice_3D_alt(output,target,eps = 1e-07):\n",
        "  \"\"\"\n",
        "  Assumes output is 3D and target is 3D\n",
        "  \"\"\"\n",
        "  per_class_dice = torch.zeros(14)\n",
        "  per_class_counts = torch.zeros(14)\n",
        "  ones = torch.ones(target[0].shape)\n",
        "  zeros = torch.zeros(target[0].shape)\n",
        "\n",
        "  for idx in range(0,14):\n",
        "    true_pos = torch.where(target == idx,ones,zeros)\n",
        "    pred_pos = torch.where(output == idx,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos)\n",
        "    denominator = true_pos + pred_pos\n",
        "\n",
        "    per_class_dice[idx] = (numerator.sum()+eps)/(denominator.sum()+eps)\n",
        "    per_class_counts[idx] = true_pos.sum()\n",
        "  \n",
        "  # ignore 0s\n",
        "  total_dice = torch.sum(torch.mul(per_class_dice[1:].float(),per_class_counts[1:]) / torch.numel(target))\n",
        "  return per_class_dice,total_dice\n",
        "\n",
        "def single_class_dice_3D(output,target,threshold,idx = 1,eps=1e-07):\n",
        "    ones = torch.ones(target.shape)\n",
        "    zeros = torch.zeros(target.shape)\n",
        "    true_pos = torch.where(target == idx,ones,zeros)\n",
        "    pred_pos = torch.where(output > threshold,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos) + eps\n",
        "    denominator = true_pos + pred_pos + eps\n",
        "\n",
        "    dice = (numerator.sum())/denominator.sum()\n",
        "    return dice\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIw8X9XLQ5DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dict = {\n",
        "    0:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ0_e7.pt\",\n",
        "    1:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ1_e120.pt\",\n",
        "    2:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ2_e119.pt\",\n",
        "    3:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ3_e14.pt\",\n",
        "    4:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ4_e2.pt\",\n",
        "    5:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ5_e26.pt\",\n",
        "    6:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ6_e45.pt\",\n",
        "    7:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ7_e2.pt\",\n",
        "    8:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ8_e26.pt\",\n",
        "    9:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ9_e37.pt\",\n",
        "    10:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ10_e80.pt\",\n",
        "    11:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ11_e33.pt\",\n",
        "    12:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ12_e46.pt\",\n",
        "    13:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ13_e34.pt\"\n",
        "\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHe25a0mO5Mg",
        "colab_type": "code",
        "outputId": "f71e7a9c-7609-4265-d209-a2a23f79f631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.cuda.empty_cache()   \n",
        "\n",
        "model = UNet()\n",
        "print (\"Model loaded.\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "checkpoint = torch.load(checkpoint_dict[6])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "try:\n",
        "  dataset\n",
        "except:\n",
        "  dataset = Nifti_Dataset(mode = \"train\",dim = 2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A-2wC-2O6b4",
        "colab_type": "text"
      },
      "source": [
        "## Create Datasets from all Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfaViSIsPGZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  file_path = \"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/\"\n",
        "  samples_per_file = 100000\n",
        "  #sample_points = np.zeros([samples_per_file*dataset.len_3d(),18])\n",
        "  #sample_points_avg5 = np.zeros([samples_per_file*dataset.len_3d(),18])\n",
        "  sample_points_avg9 = np.zeros([samples_per_file*dataset.len_3d(),18])\n",
        "  completed = 0\n",
        "\n",
        "  for idx in range(0,dataset.len_3d()):\n",
        "    x,y,mode,name = dataset.get_3d_array(idx)\n",
        "    \n",
        "    if mode == \"train\":\n",
        "      result = segment(model,checkpoint_dict,x,device)\n",
        "      #result_avg5 = F.avg_pool3d(result.unsqueeze(0),5,stride = 1,padding = 2).squeeze(0)\n",
        "      result_avg9 = F.avg_pool3d(result.unsqueeze(0),9,stride = 1,padding = 4).squeeze(0)\n",
        "\n",
        "      #file_name = file_path + \"train_file{}.pt\".format(idx)\n",
        "      #torch.save(result,file_name)\n",
        "      print(\"Saved results for train file {}\".format(idx))\n",
        "\n",
        "\n",
        "      # randomly sample points from result and append to numpy array\n",
        "      for count in range(0,samples_per_file):\n",
        "        i = np.random.randint(0,result.shape[1])\n",
        "        j = np.random.randint(0,result.shape[2])\n",
        "        k = np.random.randint(0,result.shape[3])\n",
        "\n",
        "        for cls in range(0,14):\n",
        "          #sample_points[completed*samples_per_file + count, cls] =  result[cls,i,j,k]\n",
        "          #sample_points_avg5[completed*samples_per_file + count, cls] =  result_avg5[cls,i,j,k]\n",
        "          sample_points_avg9[completed*samples_per_file + count, cls] =  result_avg9[cls,i,j,k]\n",
        "\n",
        "        #sample_points[completed*samples_per_file + count, 14] = i\n",
        "        #sample_points[completed*samples_per_file + count, 15] = j\n",
        "        #sample_points[completed*samples_per_file + count, 16] = k\n",
        "        #sample_points[completed*samples_per_file + count, 17] = y[i,j,k]\n",
        "\n",
        "        #sample_points_avg5[completed*samples_per_file + count, 14] = i\n",
        "        #sample_points_avg5[completed*samples_per_file + count, 15] = j\n",
        "        #sample_points_avg5[completed*samples_per_file + count, 16] = k\n",
        "        #sample_points_avg5[completed*samples_per_file + count, 17] = y[i,j,k]\n",
        "\n",
        "        sample_points_avg9[completed*samples_per_file + count, 14] = i\n",
        "        sample_points_avg9[completed*samples_per_file + count, 15] = j\n",
        "        sample_points_avg9[completed*samples_per_file + count, 16] = k\n",
        "        sample_points_avg9[completed*samples_per_file + count, 17] = y[i,j,k]\n",
        "\n",
        "      completed += 1\n",
        "      print(\"Sampled points for train file {}\".format(idx))\n",
        "\n",
        "      #np.save(file_path + \"sample_points{}.npy\".format(idx),sample_points)\n",
        "      #np.save(file_path + \"sample_points_avg5_{}.npy\".format(idx),sample_points_avg5)\n",
        "      np.save(file_path + \"sample_points_avg9_{}.npy\".format(idx),sample_points_avg9)\n",
        "\n",
        "      print(\"Saved copy of sample points.\")\n",
        "      \n",
        "      del result_avg9\n",
        "      del result\n",
        "      del y\n",
        "    \n",
        "    elif mode == \"\": \n",
        "      pass  \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy9f61bQRmaV",
        "colab_type": "text"
      },
      "source": [
        "## Fit Decision Trees to Each"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkfOUhWCRq8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import _pickle as pickle      \n",
        "\n",
        "if False:    \n",
        "  # fit first dataset\n",
        "  all_data = np.load(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/sample_points.npy\")\n",
        "  tree = DecisionTreeClassifier()\n",
        "  x = all_data[:,:17]\n",
        "  y = all_data[:,17]\n",
        "  tree.fit(x,y)\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/decision_tree.cpkl\",'wb') as f:\n",
        "    pickle.dump(tree,f)\n",
        "  del all_data\n",
        "\n",
        "if False:\n",
        "  # fit second dataset\n",
        "  all_data = np.load(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/sample_points_avg5.npy\")\n",
        "  tree = DecisionTreeClassifier()\n",
        "  x = all_data[:,:17]\n",
        "  y = all_data[:,17]\n",
        "  tree.fit(x,y)\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/decision_tree_avg5.cpkl\",'wb') as f:\n",
        "    pickle.dump(tree,f)\n",
        "  del all_data\n",
        "\n",
        "if True:\n",
        "  # fit third dataset\n",
        "  all_data = np.load(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/sample_points_avg9.npy\")\n",
        "  tree = DecisionTreeClassifier()\n",
        "  x = all_data[:,:17]\n",
        "  y = all_data[:,17]\n",
        "  tree.fit(x,y)\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/decision_tree_avg9.cpkl\",'wb') as f:\n",
        "    pickle.dump(tree,f)\n",
        "  del all_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E89jhMFShMw",
        "colab_type": "text"
      },
      "source": [
        "## Get results on Training Dataset\n",
        "Load each training file and predict labels with each of the following 8 strategies. Maintain running scores for each strategy, and report scores at the end.\n",
        "- Base Model (threshold 0.8)\n",
        "- Base Model with Average Pooling Kernel size 5 (threshold 0.5, 0.75)\n",
        "- Base Model with Average Pooling Kernel size 9 (threshold 0.45, 0.75)\n",
        "- DT \n",
        "- DT with Average Pooling Kernel size 5\n",
        "- DT with Average Pooling Kernel size 9\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEebYBEWTz24",
        "colab_type": "code",
        "outputId": "c62552a0-5d41-4625-b795-cc735676fdc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "all_results = {\n",
        "    \"base\":[],\n",
        "    \"base_avg5_0.5\":[],\n",
        "    \"base_avg5_0.75\":[],\n",
        "    \"base_avg9_0.45\":[],\n",
        "    \"base_avg9_0.75\":[],\n",
        "    \"dt\":[],\n",
        "    \"dt_avg5\":[],\n",
        "    \"dt_avg9\":[]\n",
        "}\n",
        "\n",
        "if True:\n",
        "  for idx in range(0,dataset.len_3d()):\n",
        "      print(\"Processing file {}\".format(idx))\n",
        "      x,y,mode,name = dataset.get_3d_array(idx)\n",
        "      \n",
        "      if mode == \"val\":\n",
        "\n",
        "        \n",
        "        result = segment(model,checkpoint_dict,x,device)\n",
        "        all_results[\"base\"].append(Dice_3D(result,y))\n",
        "        print(\"Got base result\")\n",
        "\n",
        "\n",
        "        result_avg5 = F.avg_pool3d(result.unsqueeze(0),5,stride = 1,padding = 2).squeeze(0)\n",
        "        all_results[\"base_avg5_0.5\"].append(Dice_3D(result_avg5,y,threshold = 0.5))\n",
        "        all_results[\"base_avg5_0.75\"].append(Dice_3D(result_avg5,y,threshold = 0.75))\n",
        "        print(\"Got base avgpooled results\")\n",
        "        del result_avg5\n",
        "\n",
        "        #with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/decision_tree_avg5.cpkl\",'rb') as f:\n",
        "        #  tree = pickle.load(f)\n",
        "        #result_dt_avg5 = dt_refine(result_avg5,tree)\n",
        "        #all_results[\"dt_avg5\"].append(Dice_3D_alt(result_dt_avg5,y))\n",
        "        #del result_avg5\n",
        "        #del result_dt_avg5\n",
        "        #print(\"Got dt avgpooled results\")\n",
        "\n",
        "        result_avg9 = F.avg_pool3d(result.unsqueeze(0),9,stride = 1,padding = 4).squeeze(0)\n",
        "        all_results[\"base_avg9_0.45\"].append(Dice_3D(result_avg9,y,threshold = 0.45))\n",
        "        all_results[\"base_avg9_0.75\"].append(Dice_3D(result_avg9,y,threshold = 0.75))\n",
        "        del result\n",
        "        del result_avg9\n",
        "\n",
        "          #with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/decision_tree_avg9.cpkl\",'rb') as f:\n",
        "          #  tree = pickle.load(f)\n",
        "          #result_dt_avg9 = dt_refine(result_avg9,tree)\n",
        "          #all_results[\"dt_avg9\"].append(Dice_3D_alt(result_dt_avg9,y))\n",
        "          #del result_avg9\n",
        "          #del result_dt_avg9\n",
        "    \n",
        "        \n",
        "      elif mode == \"train\":\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing file 0\n",
            "Processing file 1\n",
            "Processing file 2\n",
            "Processing file 3\n",
            "Processing file 4\n",
            "Processing file 5\n",
            "Processing file 6\n",
            "Processing file 7\n",
            "Processing file 8\n",
            "Processing file 9\n",
            "Processing file 10\n",
            "Processing file 11\n",
            "Processing file 12\n",
            "Processing file 13\n",
            "Processing file 14\n",
            "Processing file 15\n",
            "Processing file 16\n",
            "Processing file 17\n",
            "Processing file 18\n",
            "Processing file 19\n",
            "Processing file 20\n",
            "Processing file 21\n",
            "Processing file 22\n",
            "Processing file 23\n",
            "Processing file 24\n",
            "Processing file 25\n",
            "Processing file 26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Got base result\n",
            "Got base avgpooled results\n",
            "Processing file 27\n",
            "Got base result\n",
            "Got base avgpooled results\n",
            "Processing file 28\n",
            "Got base result\n",
            "Got base avgpooled results\n",
            "Processing file 29\n",
            "Got base result\n",
            "Got base avgpooled results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9dh6Tk78Z3g",
        "colab": {}
      },
      "source": [
        "# Parse all_results\n",
        "\n",
        "for key in all_results:\n",
        "  result = all_results[key]\n",
        "  try:\n",
        "    running_total = torch.zeros(result[0][0].shape)\n",
        "    count = 0\n",
        "    for item in result:\n",
        "      running_total += item[0]\n",
        "      count+=1\n",
        "    \n",
        "    avg_results = running_total / count\n",
        "\n",
        "    all_results[key] = avg_results\n",
        "  except:\n",
        "    pass\n",
        "  #print(key, all_results[key])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3X04Ws31K06",
        "colab_type": "code",
        "outputId": "73d8e3a9-5109-4ba8-9130-275a8a551adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "for key in all_results:\n",
        "  print(key,all_results[key])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base tensor([0.9881, 0.6069, 0.0096, 0.8458, 0.5231, 0.5941, 0.8153, 0.0486, 0.7745,\n",
            "        0.6009, 0.5919, 0.3361, 0.4871, 0.3413])\n",
            "base_avg5_0.5 tensor([0.9877, 0.7234, 0.0025, 0.8551, 0.4902, 0.5810, 0.8481, 0.0543, 0.8093,\n",
            "        0.6863, 0.4647, 0.4102, 0.2722, 0.2865])\n",
            "base_avg5_0.75 tensor([9.6354e-01, 6.9975e-01, 1.1105e-04, 6.8099e-01, 2.7119e-01, 3.8267e-01,\n",
            "        8.0533e-01, 6.8593e-13, 7.2050e-01, 6.2072e-01, 1.8807e-01, 2.9839e-01,\n",
            "        4.7550e-02, 8.4411e-02])\n",
            "base_avg9_0.45 tensor([9.8671e-01, 7.6390e-01, 6.3435e-13, 8.2418e-01, 3.5322e-01, 4.8172e-01,\n",
            "        8.6415e-01, 5.1004e-02, 8.1755e-01, 7.1167e-01, 2.0717e-01, 3.7460e-01,\n",
            "        9.6815e-03, 1.0621e-01])\n",
            "base_avg9_0.75 tensor([9.5002e-01, 5.7018e-01, 1.0975e-12, 4.4801e-01, 4.5906e-02, 9.5308e-02,\n",
            "        7.3506e-01, 6.8593e-13, 5.4302e-01, 4.9191e-01, 3.2134e-02, 1.7405e-01,\n",
            "        7.5296e-11, 1.3124e-02])\n",
            "dt []\n",
            "dt_avg5 []\n",
            "dt_avg9 []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DH3HrIASPH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/all_results.cpkl\",'wb') as f:\n",
        "    pickle.dump(all_results,f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}