{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2D Seg - Evaluation and Ensembling",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerekGloudemans/segmentation-medical-images/blob/master/2D_Seg_Evaluation_and_Ensembling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9pBYUcvavJN",
        "colab_type": "text"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWuPYaAANe9r",
        "colab_type": "code",
        "outputId": "34e36478-1324-488c-81eb-262343726031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qqHuf8NN6hw",
        "colab_type": "code",
        "outputId": "14baac73-4f65-41e5-c878-a65387af3fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#%%capture \n",
        "!pip install -q --upgrade ipython==5.5.0\n",
        "!pip install -q --upgrade ipykernel==4.6.0\n",
        "!pip3 install torchvision\n",
        "!pip3 install opencv-python\n",
        "\n",
        "import ipywidgets\n",
        "import traitlets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▏                            | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAi-OI1_N65G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "\n",
        "# this seems to be a popular thing to do so I've done it here\n",
        "#from __future__ import print_function, division\n",
        "\n",
        "\n",
        "# torch and specific torch packages for convenience\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils import data\n",
        "from torch import multiprocessing\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# for convenient data loading, image representation and dataset management\n",
        "from torchvision import models, transforms\n",
        "import torchvision.transforms.functional as FT\n",
        "from PIL import Image, ImageFile, ImageStat\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from scipy.ndimage import affine_transform\n",
        "import cv2\n",
        "\n",
        "# always good to have\n",
        "import time\n",
        "import os\n",
        "import numpy as np    \n",
        "import _pickle as pickle\n",
        "import random\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import nibabel as nib\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpph35XWQIC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        n_classes=1,\n",
        "        depth=3,\n",
        "        wf=4,\n",
        "        padding=True,\n",
        "        batch_norm=False,\n",
        "        up_mode='upconv',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(\n",
        "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "        innum = 1000\n",
        "        midnum = 100\n",
        "        outnum = 4*n_classes\n",
        "        self.reg = nn.Sequential(\n",
        "            nn.BatchNorm1d(innum),\n",
        "            nn.Linear(innum,midnum),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(midnum,outnum),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #for param in self.parameters():\n",
        "        #  param.requires_grad = True\n",
        "\n",
        "    def forward(self, x,BBOX = False):\n",
        "        blocks = []\n",
        "        \n",
        "        # encoder\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path) - 1:\n",
        "                blocks.append(x)\n",
        "                x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # do bbox regression here\n",
        "        if BBOX:\n",
        "          x_reg = x.view(-1)\n",
        "          x_reg = self.reg(x_reg)\n",
        "          bboxes = x_reg.view(4,-1)\n",
        "\n",
        "        # decoder\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "\n",
        "        #CHANGE THIS LINE FOR MULTIPLE OUTPUT CHANNELS\n",
        "        # apply per_class last layer and per-class Softmax \n",
        "        #x = nn.Softmax2d(self.last(x)) \n",
        "        x = torch.sigmoid(self.last(x))\n",
        "        \n",
        "        if BBOX:\n",
        "          return x, bboxes\n",
        "        else:\n",
        "          return x\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
        "            )\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[\n",
        "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oGORnSYOTan",
        "colab_type": "text"
      },
      "source": [
        "## Define Dataset for Dealing with NIfTI Files\n",
        "(1) spleen\n",
        "(2) right kidney\n",
        "(3) left kidney\n",
        "(4) gallbladder\n",
        "(5) esophagus\n",
        "(6) liver\n",
        "(7) stomach\n",
        "(8) aorta\n",
        "(9) inferior vena cava\n",
        "(10) portal vein and splenic vein\n",
        "(11) pancreas\n",
        "(12) right adrenal gland\n",
        "(13) left adrenal gland"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxDtlv1qfaoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Nifti_Dataset(data.Dataset):\n",
        "  def __init__(self,mode = \"view\",dim = 2,class_id = 1):\n",
        "    \"\"\"\n",
        "    Save the last 0.15 proportion of files after sorting for use as validation set.\n",
        "    Loads each slice of the input images as a separate image\n",
        "    mode - view,train or val, defined in same dataset to maintain data separation\n",
        "      view - performs transforms but does not normalize images\n",
        "      train - normalizes data and performs transforms\n",
        "      val - normalizes data, no augmenting transforms\n",
        "    dim - specifies dimension along which to slice image\n",
        "    \"\"\"\n",
        "\n",
        "    self.mode = mode\n",
        "    self.dim = dim\n",
        "    self.class_id = class_id\n",
        "\n",
        "    data_dir = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Training/img\"\n",
        "    label_dir = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Training/label\"\n",
        "\n",
        "    # get all data and label file names\n",
        "    self.data_files = []\n",
        "    for file in os.listdir(data_dir):\n",
        "      self.data_files.append(os.path.join(data_dir,file))\n",
        "    self.data_files.sort()\n",
        "\n",
        "    self.label_files = []\n",
        "    for file in os.listdir(label_dir):\n",
        "      self.label_files.append(os.path.join(label_dir,file))\n",
        "    self.label_files.sort()\n",
        "\n",
        "    # for each data_file\n",
        "    self.train_data = []\n",
        "    self.val_data = []\n",
        "\n",
        "    for i in range(len(self.data_files)):\n",
        "      data = nib.load(self.data_files[i])\n",
        "      data = np.array(data.get_fdata())\n",
        "\n",
        "      label = nib.load(self.label_files[i])\n",
        "      label = np.array(label.get_fdata()).astype(float)\n",
        "\n",
        "      identifier = self.data_files[i].split(\"_\")[0]\n",
        "      for slice in range(0,data.shape[dim]):\n",
        "\n",
        "        # get slices\n",
        "        if dim == 0:\n",
        "          data_slice = data[slice,:,:]\n",
        "          label_slice = label[slice,:,:]\n",
        "        elif dim == 1:\n",
        "          data_slice = data[:,slice,:]\n",
        "          label_slice = label[:,slice,:]\n",
        "        elif dim == 2:\n",
        "          data_slice = data[:,:,slice]\n",
        "          label_slice = label[:,:,slice]\n",
        "\n",
        "        mean,std = np.mean(data_slice),np.std(data_slice)\n",
        "        # define item dict to store info\n",
        "        item = {\n",
        "            \"identifier\":identifier,\n",
        "            \"slice\":slice,\n",
        "            \"data\":data_slice,\n",
        "            \"label\":label_slice,\n",
        "            \"mean\":mean,\n",
        "            \"std\":std\n",
        "            }\n",
        "\n",
        "        # check to make sure this example actually has organs in it\n",
        "        test = np.bincount(label_slice.astype(int).reshape(-1))\n",
        "        if len(test) == 1: \n",
        "          continue\n",
        "\n",
        "        # make sure 50% of examples have organ of interest in them\n",
        "        if class_id not in np.unique(label_slice.astype(int).reshape(-1)):\n",
        "            if np.random.rand() > 0.5:\n",
        "              continue\n",
        "\n",
        "        # assign to either training or validation data\n",
        "        if i < len(self.data_files) * 0.85:\n",
        "          self.train_data.append(item)\n",
        "        else:\n",
        "          self.val_data.append(item)\n",
        "\n",
        "      #break # to shorten loading time\n",
        "\n",
        "    # define some transforms for training dataset\n",
        "    self.train_transforms = transforms.Compose([\n",
        "          transforms.ColorJitter(brightness = 0.2,contrast = 0.2,saturation = 0.1),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.RandomErasing(p=0.0015, scale=(0.4, 0.6), ratio=(0.3, 3.3), value=0, inplace=False), # big\n",
        "          transforms.RandomErasing(p=0.003, scale=(0.1, 0.3), ratio=(0.3, 3.3), value=0, inplace=False), # medium\n",
        "          transforms.RandomErasing(p=0.004, scale=(0.05, 0.15), ratio=(0.3, 3.3), value=0, inplace=False),# small\n",
        "          transforms.RandomErasing(p=0.0035, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=0, inplace=False) # small\n",
        "          \n",
        "\n",
        "        ])\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      #get relevant dictionary from self memory\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        item = self.train_data[index]\n",
        "      else:\n",
        "        item = self.val_data[index]\n",
        "\n",
        "      x = Image.fromarray(item['data']).copy()\n",
        "      y = Image.fromarray(item['label']).copy()\n",
        "\n",
        "      # to grayscale\n",
        "      x  = FT.to_grayscale(x)\n",
        "      y = FT.to_grayscale(y)\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        # randomly flip and rotate both\n",
        "        FLIP = 0 #np.random.rand()\n",
        "        if FLIP > 0.5:\n",
        "          x = FT.hflip(x)\n",
        "          y = FT.hflip(y)\n",
        "\n",
        "        ROTATE = 0 #np.random.rand()*60 - 30\n",
        "        x  = x.rotate(ROTATE)\n",
        "        y = y.rotate(ROTATE,Image.NEAREST)\n",
        "\n",
        "      # resize to 224 on shorter dimension\n",
        "      x = FT.resize(x, 256)\n",
        "      y = FT.resize(y,256,Image.NEAREST)\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        # randomly jitter color of data and randomly erase data\n",
        "        x = self.train_transforms(x)\n",
        "      # to tensor\n",
        "      try:\n",
        "        x = FT.to_tensor(x)\n",
        "      except:\n",
        "        pass\n",
        "      y = FT.to_tensor(y)\n",
        "            \n",
        "      # normalize and repeat along color dimension if in train or val mode\n",
        "      if self.mode in ['train','val']:\n",
        "        #x = FT.normalize(x,[item['mean']],[item['std']])\n",
        "        #x = x.repeat(3,1,1)\n",
        "        pass\n",
        "\n",
        "      return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.mode in [\"train\",\"view\"]:\n",
        "      return len(self.train_data)\n",
        "    else:\n",
        "      return len(self.val_data)\n",
        "\n",
        "  def show(self,index):\n",
        "    data,label = self[index]\n",
        "    plt.figure()\n",
        "    plt.subplot(121)\n",
        "    data = data.detach()\n",
        "    plt.imshow(data[0],cmap = \"gray\")\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(label[0],cmap = \"gray\")\n",
        "    plt.show()\n",
        "    # convert each tensor to numpy array\n",
        "\n",
        "  def show_slices(self,idx = 0,dim = 0,organ_id = None):\n",
        "    \"\"\"\n",
        "    A nice utility function for plotting all of the slices along a given dimension\n",
        "    idx - indexes all NIfTI images in dataset\n",
        "    dim - indexes dimension of image\n",
        "    organ_id - if not None, all other organs removed from label\n",
        "    \"\"\"\n",
        "    data = nib.load(self.data_files[idx])\n",
        "    label = nib.load(self.label_files[idx])\n",
        "\n",
        "    data = data.get_fdata()\n",
        "    data = np.array(data)\n",
        "    label = label.get_fdata()\n",
        "    label = np.array(label)\n",
        "\n",
        "    for slice in range(0,data.shape[dim]):\n",
        "      if dim == 0:\n",
        "            data_slice = data[slice,:,:]\n",
        "            label_slice = label[slice,:,:]\n",
        "      elif dim == 1:\n",
        "        data_slice = data[:,slice,:]\n",
        "        label_slice = label[:,slice,:]\n",
        "      elif dim == 2:\n",
        "        data_slice = data[:,:,slice]\n",
        "        label_slice = label[:,:,slice]\n",
        "\n",
        "      if organ_id is not None:\n",
        "        # if a specific label is to be looked at, 0 all others\n",
        "        label_slice = 1.0 - np.ceil(np.abs(label_slice.astype(float)-organ_id)/15.0)\n",
        "\n",
        "      print(np.unique(label_slice))\n",
        "      plt.figure()\n",
        "      plt.subplot(121)\n",
        "      plt.imshow(data_slice,cmap = \"gray\")\n",
        "\n",
        "      plt.subplot(122)\n",
        "      plt.imshow(label_slice,cmap = \"gray\")\n",
        "      plt.show()\n",
        "\n",
        "  def len_3d(self):\n",
        "    return len(self.data_files)\n",
        "\n",
        "  def get_3d_array(self,idx):\n",
        "      \"\"\"\n",
        "      Loads a 3D image as a tensor as well as its label, mode, and file name\n",
        "      \"\"\"\n",
        "      assert idx < len(self.data_files) , \"3D image index out of range, there are {} 3D images\".format(len(self.data_files))\n",
        "\n",
        "      # load data and label as tensors\n",
        "      data = nib.load(self.data_files[idx])\n",
        "      label = nib.load(self.label_files[idx])\n",
        "\n",
        "      data = data.get_fdata()\n",
        "      data = torch.from_numpy(np.array(data))\n",
        "      label = label.get_fdata()\n",
        "      label = torch.from_numpy(np.array(label)).int()\n",
        "\n",
        "      # note whether image is training or validation set\n",
        "      if idx < 0.85 * len(self.data_files):\n",
        "        mode = \"train\"\n",
        "      else:\n",
        "        mode = \"val\"\n",
        "      \n",
        "      return data,label,mode,self.data_files[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TUO8-ldNvCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def refactor_inputs(x,y,mode = \"all_organs\",id_num = 1,num_outputs = 1):\n",
        "  \"\"\"\n",
        "  Modifies inputs for different segmentation-style tasks with the goal of training\n",
        "  the same network for progressively more difficult tasks\n",
        "  mode - foreground,all_organs,edges,organ_slices, bboxes \n",
        "  x - batch x 1 x 256x256 image\n",
        "  y - batch x 1 x 256 x 256 image with integer value classes\n",
        "  id_num - int - specifies organ to look for for single organ mode\n",
        "  num_outputs - specifies number of output maps to make\n",
        "  \"\"\"\n",
        "  ones = torch.ones(y.shape)\n",
        "  zeros = torch.zeros(y.shape)\n",
        "  y = (y/0.0039).floor().int()\n",
        "\n",
        "  if mode == \"foreground\":\n",
        "    # all foreground is 1, everything else is 0\n",
        "    # all slices but first share same info\n",
        "    threshold = x.mean()\n",
        "    y1 = torch.where(x > threshold,ones,zeros)\n",
        "    # fills all labeled areas with ones as well\n",
        "    y1 = torch.where(y>0,ones,y1)\n",
        "    target = y1.repeat(1,num_outputs,1,1)\n",
        "\n",
        "  elif mode == \"all_organs\":\n",
        "    # all organs are one, everything else is 0\n",
        "    positive_class = 0\n",
        "    y1 = torch.where(y > 0,ones, zeros)\n",
        "    # get same map for each class\n",
        "    target = y1.repeat(1,num_outputs,1,1)\n",
        "\n",
        "  elif mode == \"single_organ\":\n",
        "    # all slices contain same map (positive for a single organ)\n",
        "    y1 = torch.where((y == id_num),ones,zeros)\n",
        "    target = y1#.repeat(1,num_outputs,1,1)\n",
        "\n",
        "  elif mode == \"edges\":\n",
        "    pooled = F.avg_pool2d(y,3,stride =1,padding = 1 )\n",
        "    y1 = torch.abs(pooled - y)\n",
        "    y1 = torch.where(y1>0,ones,zeros)\n",
        "    target = y1.repeat(1,num_outputs,1,1)\n",
        "\n",
        "  elif mode == \"x_edges\":\n",
        "    x_norm = torch.where(x>0,ones,zeros)\n",
        "    pooled = F.avg_pool2d(x_norm,3,stride =1,padding = 1 )\n",
        "    y1 = torch.abs(pooled - x_norm)\n",
        "    y1 = torch.where(y1>0.1,ones,zeros)\n",
        "    target = y1.repeat(1,num_outputs,1,1)\n",
        "\n",
        "  elif mode == \"bboxes\":\n",
        "\n",
        "    indices = y.nonzero()\n",
        "    target = torch.zeros((y.shape[0],13,4)) # batch idx, class idx, xmin ymin xmax ymax\n",
        "    target[:,:,:2] = 100000 # set min to greater than \n",
        "    for item in indices:\n",
        "      batch_idx = item[0]\n",
        "      x_val = item[3]\n",
        "      y_val = item[2]\n",
        "      class_idx = int(y[batch_idx,0,y_val,x_val]) -1\n",
        "\n",
        "      if x_val < target[batch_idx,class_idx,0]:\n",
        "        target[batch_idx,class_idx,0] = x_val\n",
        "      if x_val > target[batch_idx,class_idx,2]:\n",
        "        target[batch_idx,class_idx,2] = x_val\n",
        "      if y_val < target[batch_idx,class_idx,1]:\n",
        "        target[batch_idx,class_idx,1] = y_val\n",
        "      if y_val > target[batch_idx,class_idx,3]:\n",
        "        target[batch_idx,class_idx,3] = y_val  \n",
        "\n",
        "  elif mode == \"all_to_one\":\n",
        "    # all slices contain same map (positive for a single organ)\n",
        "    y1 = torch.where((y.int() == id_num),ones,zeros)\n",
        "    target = y1#.repeat(1,num_outputs,1,1)\n",
        "     \n",
        "    positive_class = 0\n",
        "    x1 = torch.where(y > 0,ones, zeros)\n",
        "    # get same map for each class\n",
        "    x = x1.repeat(1,num_outputs,1,1)\n",
        "\n",
        "  return x,target\n",
        "\n",
        "def test_slice(dataset,model,idx,device,refactor_mode,id_num = 1):\n",
        "  x,y = dataset[idx]\n",
        "  x = x.unsqueeze(0)\n",
        "  y = y.unsqueeze(0)\n",
        "\n",
        "  x,y_new = refactor_inputs(x,y,mode = refactor_mode,id_num = id_num)\n",
        "  x = x.to(device)\n",
        "\n",
        "  output = model(x)\n",
        "  output = output.cpu()\n",
        "  output = output.data.numpy()\n",
        "  \n",
        "  # inputs\n",
        "  plt.figure(figsize = (10,10))\n",
        "  plt.subplot(221)\n",
        "  plt.title(\"Inputs\")\n",
        "  inputs = x.data.cpu().numpy()[0][0]\n",
        "  plt.imshow(inputs,cmap = \"gray\")\n",
        "  plt.clim(0,1)\n",
        "\n",
        "  # original labels\n",
        "  plt.subplot(222)\n",
        "  plt.imshow(y[0][0],cmap = \"gray\")\n",
        "  plt.title(\"Original Labels\")\n",
        "\n",
        "\n",
        "  # outputs\n",
        "  plt.subplot(223)\n",
        "  plt.imshow(output[0][0],cmap = \"gray\")\n",
        "  plt.clim(0,1)\n",
        "  plt.title(\"Outputs\")\n",
        " \n",
        "  # binary target\n",
        "  if refactor_mode == \"bboxes\":\n",
        "    # binary target\n",
        "    plt.subplot(224)\n",
        "    copy = y[0][0].data.numpy().copy()\n",
        "    for box in y_new[0]:\n",
        "      if box[0] < 10000:\n",
        "        corner1 = (box[0],box[1])\n",
        "        corner2 = ((box[2],(box[3])))\n",
        "        copy = cv2.rectangle(copy,corner1,corner2,(0.1),1)\n",
        "      #break\n",
        "\n",
        "    plt.imshow(copy,cmap = \"gray\")\n",
        "    plt.title(\"Correct Labels\")\n",
        "    #plt.clim(0,13)\n",
        "    plt.show()\n",
        "\n",
        "  else:\n",
        "    plt.subplot(224)\n",
        "    plt.imshow(y_new[0][0],cmap = \"gray\")\n",
        "    plt.title(\"Correct Labels\")\n",
        "    #plt.clim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "  return inputs, output[0][0]\n",
        "\n",
        "class Dice_Loss(nn.Module):        \n",
        "    def __init__(self):\n",
        "        super(Dice_Loss,self).__init__()\n",
        "        \n",
        "    def forward(self,output,target,epsilon = 1e-07):\n",
        "        \"\"\" Compute the bbox iou loss for target vs output using tensors to preserve\n",
        "        gradients for efficient backpropogation\"\"\"\n",
        "\n",
        "        numerator = 2.0*torch.mul(output,target) \n",
        "        denominator = output + target\n",
        "        dice = torch.div(numerator,denominator)\n",
        "        #print (dice.shape)\n",
        "        #loss =  1.0 -(torch.sum(dice)/torch.numel(dice))\n",
        "        #print(\"{} / {} = {}\".format(numerator.sum(),denominator.sum(),loss))\n",
        "\n",
        "        loss = 1.0 - torch.sum(numerator)/torch.sum(denominator)\n",
        "        #print (loss.shape)\n",
        "        #print (loss)\n",
        "        return loss\n",
        "\n",
        "def load_model(checkpoint_file,model,optimizer):\n",
        "  \"\"\"\n",
        "  Reloads a checkpoint, loading the model and optimizer state_dicts and \n",
        "  setting the start epoch\n",
        "  \"\"\"\n",
        "  checkpoint = torch.load(checkpoint_file)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  all_losses = checkpoint['losses']\n",
        "  all_accs = checkpoint['accs']\n",
        "\n",
        "  return model,optimizer,epoch,all_losses,all_accs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_3RVZAbBKrA",
        "colab_type": "text"
      },
      "source": [
        "## Define Assorted Functions for Training and Testing\n",
        "\n",
        "Critical to realize here is that we can either define a UNet that has 1 output channel (positive negative) or 13 output channels (each class). For now, we'll start with just one output map, and see where that gets us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x9n2OQoGsL8",
        "colab_type": "text"
      },
      "source": [
        "## Main Code Body\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rqRsjdYp6BH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dict = {\n",
        "    0:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ0_e7.pt\",\n",
        "    1:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ1_e120.pt\",\n",
        "    2:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ2_e119.pt\",\n",
        "    3:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ3_e14.pt\",\n",
        "    4:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ4_e2.pt\",\n",
        "    5:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ5_e26.pt\",\n",
        "    6:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ6_e45.pt\",\n",
        "    7:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ7_e2.pt\",\n",
        "    8:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ8_e26.pt\",\n",
        "    9:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ9_e37.pt\",\n",
        "    10:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ10_e80.pt\",\n",
        "    11:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ11_e33.pt\",\n",
        "    12:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ12_e46.pt\",\n",
        "    13:\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/best_UNET11_organ13_e34.pt\"\n",
        "\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pAg5ZnEkMax",
        "colab_type": "code",
        "outputId": "66ce419d-0000-472b-f2b6-32605322f8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "  # CUDA for PyTorch\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "  torch.cuda.empty_cache()   \n",
        "  \n",
        "  model = UNet()\n",
        "  print (\"Model loaded.\")\n",
        "  model = model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  checkpoint = torch.load(checkpoint_dict[6])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH5TipE9yXa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMcEpD5uPFEx",
        "colab_type": "code",
        "outputId": "1eceb70f-0eb0-4c9d-d359-b6badfec213d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "try:\n",
        "  dataset\n",
        "except:\n",
        "  dataset = Nifti_Dataset(mode = \"train\",dim = 2)\n",
        "\n",
        "idx = np.random.randint(0,len(dataset))\n",
        "dataset.show(idx)\n",
        "\n",
        "x,y,mode,name = dataset.get_3d_array(0)\n",
        "result = predict_by_slices(model,x,device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYs7dMAwjgphzNwzDiCDm3A3DMCLI/wPG9bsS5HaqUwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f5b9a14da590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_3d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_by_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_by_slices' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2HQ3PkDtc4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  count = 0\n",
        "  per_class_total = torch.zeros(14)\n",
        "  total = 0\n",
        "\n",
        "  for i in range(0,dataset.len_3d()):\n",
        "    x,y,mode,name = dataset.get_3d_array(i)\n",
        "    \n",
        "    if mode == \"val\":\n",
        "      break\n",
        "    result = segment(model,checkpoint_dict,x,device)\n",
        "\n",
        "    per_class,overall = Dice_3D(result,y)\n",
        "    print(\"Finished volume {}. Avg. Dice {} \".format(i,overall))\n",
        "\n",
        "    count = count+1\n",
        "    per_class_total += per_class\n",
        "    total += overall\n",
        "\n",
        "  per_class_total = per_class_total / count\n",
        "  total = total / count\n",
        "\n",
        "  print (\"Classwise dice accuracies: {}\".format(per_class_total))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUc2QFY2h-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  checkpoint = torch.load(checkpoint_dict[1])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  idx = np.random.randint(0,dataset.len_3d())\n",
        "  dataset.show(idx)\n",
        "\n",
        "  x,y,mode,name = dataset.get_3d_array(idx)\n",
        "  result = predict_by_slices(model,x,device)\n",
        "  result2 = F.avg_pool3d(result.unsqueeze(0).unsqueeze(0),3,stride = 1,padding = 1).squeeze(0).squeeze(0)\n",
        "  result3 = F.avg_pool3d(result.unsqueeze(0).unsqueeze(0),5,stride = 1,padding = 2).squeeze(0).squeeze(0)\n",
        "  result4 = F.avg_pool3d(result.unsqueeze(0).unsqueeze(0),7,stride = 1,padding = 3).squeeze(0).squeeze(0)\n",
        "  result5 = F.avg_pool3d(result.unsqueeze(0).unsqueeze(0),9,stride = 1,padding = 4).squeeze(0).squeeze(0)\n",
        "\n",
        "  all_dices = []\n",
        "  all_dices2 = []\n",
        "  all_dices3 = []\n",
        "  all_dices4 = []\n",
        "  all_dices5 = []\n",
        "  for threshold in np.arange(0,1,0.02):\n",
        "    all_dices.append(single_class_dice_3D(result,y,threshold,idx = 1))\n",
        "    all_dices2.append(single_class_dice_3D(result2,y,threshold,idx = 1))\n",
        "    all_dices3.append(single_class_dice_3D(result3,y,threshold,idx = 1))\n",
        "    all_dices4.append(single_class_dice_3D(result4,y,threshold,idx = 1))\n",
        "    all_dices5.append(single_class_dice_3D(result5,y,threshold,idx = 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUmWRwuFBvXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  style.use(\"fivethirtyeight\")\n",
        "  x = np.arange(0,1,0.02)\n",
        "  plt.figure(figsize = (15,15))\n",
        "  plt.plot(x,all_dices)\n",
        "  plt.plot(x,all_dices2)\n",
        "  plt.plot(x,all_dices3)\n",
        "  plt.plot(x,all_dices4)\n",
        "  plt.plot(x,all_dices5)\n",
        "  plt.legend([\"No Averaging\",\"Kernel Size 3\",\"Kernel Size 5\",\"Kernel Size 7\",\"Kernel Size 9\"])\n",
        "  plt.xlabel(\"Discrimination Threshold\")\n",
        "  plt.ylabel(\"Dice Accuracy\")\n",
        "  plt.title(\"Which Kernel is the Best?\")\n",
        "  import matplotlib.style as style\n",
        "  plt.show()\n",
        "\n",
        "  # Use 44%, kernel size = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yEfTi8kF_zO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if False:\n",
        "  file_path = \"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/\"\n",
        "  samples_per_file = 100000\n",
        "  sample_points = np.zeros([samples_per_file*dataset.len_3d(),18])\n",
        "  completed = 0\n",
        "\n",
        "  for idx in range(0,dataset.len_3d()):\n",
        "    x,y,mode,name = dataset.get_3d_array(idx)\n",
        "    \n",
        "    if mode == \"train\":\n",
        "      continue\n",
        "      result = segment(model,checkpoint_dict,x,device)\n",
        "      #file_name = file_path + \"train_file{}.pt\".format(idx)\n",
        "      #torch.save(result,file_name)\n",
        "      print(\"Saved results for train file {}\".format(idx))\n",
        "\n",
        "\n",
        "      # randomly sample points from result and append to numpy array\n",
        "      for count in range(0,samples_per_file):\n",
        "        i = np.random.randint(0,result.shape[1])\n",
        "        j = np.random.randint(0,result.shape[2])\n",
        "        k = np.random.randint(0,result.shape[3])\n",
        "\n",
        "        for cls in range(0,14):\n",
        "          sample_points[completed*samples_per_file + count, cls] =  result[cls,i,j,k]\n",
        "        sample_points[completed*samples_per_file + count, 14] = i\n",
        "        sample_points[completed*samples_per_file + count, 15] = j\n",
        "        sample_points[completed*samples_per_file + count, 16] = k\n",
        "        sample_points[completed*samples_per_file + count, 17] = y[i,j,k]\n",
        "\n",
        "      completed += 1\n",
        "      print(\"Sampled points for train file {}\".format(idx))\n",
        "\n",
        "      np.save(file_path + \"sample_points{}.npy\".format(idx),sample_points)\n",
        "      print(\"Saved copy of sample points.\")\n",
        "      \n",
        "      del result\n",
        "      del y\n",
        "    \n",
        "    elif mode == \"val\":\n",
        "      result = segment(model,checkpoint_dict,x,device)\n",
        "      #file_name = file_path + \"val_file{}.pt\".format(idx)\n",
        "      #torch.save(result,file_name) \n",
        "      print(\"Saved results for file {}\".format(idx))\n",
        "\n",
        "      for count in range(0,samples_per_file):\n",
        "        i = np.random.randint(0,result.shape[1])\n",
        "        j = np.random.randint(0,result.shape[2])\n",
        "        k = np.random.randint(0,result.shape[3])\n",
        "\n",
        "        for cls in range(0,14):\n",
        "          sample_points[completed*samples_per_file + count, cls] =  result[cls,i,j,k]\n",
        "        sample_points[completed*samples_per_file + count, 14] = i\n",
        "        sample_points[completed*samples_per_file + count, 15] = j\n",
        "        sample_points[completed*samples_per_file + count, 16] = k\n",
        "        sample_points[completed*samples_per_file + count, 17] = y[i,j,k]\n",
        "\n",
        "      completed += 1\n",
        "      print(\"Sampled points for val file {}\".format(idx))\n",
        "\n",
        "      np.save(file_path + \"val_sample_points{}.npy\".format(idx),sample_points)\n",
        "      print(\"Saved copy of val sample points.\")\n",
        "      \n",
        "      del result\n",
        "      del y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tsLyFevQcKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = np.load(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/sample_points25.npy\")\n",
        "#all_val_data = np.load(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/val_sample_points29.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DcOhoptXsRP",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "x = all_data[:,:17]\n",
        "y = all_data[:,17]\n",
        "\n",
        "#x_val = all_val_data[:,:17]\n",
        "#y_val = all_val_data[:,17]\n",
        "tree.fit(x,y)\n",
        "\n",
        "import _pickle as pickle\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/Segmentation/Save_files/decision_tree_no_avgpool.cpkl\",'wb') as f:\n",
        "  pickle.dump(tree,f)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWUQN2foZSn6",
        "colab_type": "code",
        "outputId": "c51beec2-96e5-4536-f3d4-c40caff05db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_score = tree.score(x,y)\n",
        "print (\"Training accuracy: {}\".format(train_score))\n",
        "\n",
        "#test_score = tree.score(x_val,y_val)\n",
        "#print (\"Validation accuracy: {}\".format(test_score))\n",
        "\n",
        "#out = tree.predict(x_val)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.9999996666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XMalDHaz3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.bincount(out.astype(int))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtrBpFTpcEqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dt_refine(orig,tree):\n",
        "  \"\"\"\n",
        "  Applies decision tree ensembling to original outputs\n",
        "  orig = classes x l x w x h\n",
        "  \"\"\"\n",
        "  output = torch.zeros((orig.shape[1],orig.shape[2],orig.shape[3]))\n",
        "\n",
        "  for i in range(0,orig.shape[1]):\n",
        "    print (\"On i = {}\".format(i))\n",
        "    for j in range(0,orig.shape[2]):\n",
        "      \n",
        "        inp = np.zeros([orig.shape[3],17])\n",
        "        inp[:,:14] = orig[:,i,j,:].data.numpy().transpose()\n",
        "        inp[:,14] = i\n",
        "        inp[:,15] = j\n",
        "        for k in range(0,orig.shape[3]):\n",
        "          inp[k,16] = k\n",
        "\n",
        "        output[i,j,:] = torch.from_numpy(tree.predict(inp))\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AVrBaeOjHio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_by_slices(model,data,device,outfile = None):\n",
        "  \"\"\"\n",
        "  Takes in 3D tensor, slices and segments using UNET, and returns result\n",
        "  Note that result will be a [0,1] tensor corresponding to a certain class (whatever model class was used)\n",
        "  if outfile is not none, saves results in  that file \n",
        "  \"\"\"\n",
        "\n",
        "  num_slices = data.shape[2]\n",
        "  result = torch.zeros(data.shape)\n",
        "\n",
        "  for idx in range(num_slices):\n",
        "    # resize to 256 x 256\n",
        "    slice = data[:,:,idx]\n",
        "    original_shape = slice.shape\n",
        "  \n",
        "    slice =  Image.fromarray(slice.data.numpy()).copy()\n",
        "    slice = FT.to_grayscale(slice)\n",
        "    slice = FT.to_tensor(slice)\n",
        "    slice = slice.unsqueeze(0)\n",
        "\n",
        "    slice = F.interpolate(slice,size = [256,256],mode = 'bilinear')\n",
        "\n",
        "    x = slice.to(device).float()\n",
        "\n",
        "    out_slice = model(x)\n",
        "    out_slice = F.interpolate(out_slice,original_shape,mode = 'nearest')\n",
        "    result[:,:,idx] = out_slice.data.cpu()\n",
        "  \n",
        "  if outfile:\n",
        "    torch.save(result, outfile)\n",
        "\n",
        "  return result\n",
        "\n",
        "def segment(model,checkpoint_dict,data,device,outfile = None):\n",
        "  \"\"\"\n",
        "  Takes a 3D tensor and segments it with a series of models, predicting maximum class for each\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  result = torch.zeros((14,data.shape[0],data.shape[1],data.shape[2]))\n",
        "\n",
        "  for i in range(0,14):\n",
        "    checkpoint = torch.load(checkpoint_dict[i])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    result[i,:,:,:] = predict_by_slices(model,data,device,outfile = None)\n",
        "    #print(\"Finished class {}\".format(i))\n",
        "  \n",
        "  #result = result.round()\n",
        "  #result[0,:,:,:] = result[0,:,:,:].round()*0.3\n",
        "  #result = torch.argmax(result,dim = 0)\n",
        "\n",
        "  return result\n",
        "\n",
        "def Dice_3D(output,target,eps = 1e-07):\n",
        "  \"\"\"\n",
        "  Assumes output is 4D and target is 3D\n",
        "  \"\"\"\n",
        "  per_class_dice = torch.zeros(14)\n",
        "  per_class_counts = torch.zeros(14)\n",
        "  ones = torch.ones(target[0].shape)\n",
        "  zeros = torch.zeros(target[0].shape)\n",
        "\n",
        "  for idx in range(0,14):\n",
        "    true_pos = torch.where(target == idx,ones,zeros)\n",
        "    pred_pos = torch.where(output[idx,:,:,:] > 0.5,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos)\n",
        "    denominator = true_pos + pred_pos\n",
        "\n",
        "    per_class_dice[idx] = (numerator.sum()+eps)/(denominator.sum()+eps)\n",
        "    per_class_counts[idx] = true_pos.sum()\n",
        "  \n",
        "  # ignore 0s\n",
        "  total_dice = torch.sum(torch.mul(per_class_dice[1:].float(),per_class_counts[1:]) / torch.numel(target))\n",
        "  return per_class_dice,total_dice\n",
        "\n",
        "def Dice_3D_alt(output,target,eps = 1e-07):\n",
        "  \"\"\"\n",
        "  Assumes output is 3D and target is 3D\n",
        "  \"\"\"\n",
        "  per_class_dice = torch.zeros(14)\n",
        "  per_class_counts = torch.zeros(14)\n",
        "  ones = torch.ones(target[0].shape)\n",
        "  zeros = torch.zeros(target[0].shape)\n",
        "\n",
        "  for idx in range(0,14):\n",
        "    true_pos = torch.where(target == idx,ones,zeros)\n",
        "    pred_pos = torch.where(output == idx,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos)\n",
        "    denominator = true_pos + pred_pos\n",
        "\n",
        "    per_class_dice[idx] = (numerator.sum()+eps)/(denominator.sum()+eps)\n",
        "    per_class_counts[idx] = true_pos.sum()\n",
        "  \n",
        "  # ignore 0s\n",
        "  total_dice = torch.sum(torch.mul(per_class_dice[1:].float(),per_class_counts[1:]) / torch.numel(target))\n",
        "  return per_class_dice,total_dice\n",
        "\n",
        "def single_class_dice_3D(output,target,threshold,idx = 1,eps=1e-07):\n",
        "    ones = torch.ones(target.shape)\n",
        "    zeros = torch.zeros(target.shape)\n",
        "    true_pos = torch.where(target == idx,ones,zeros)\n",
        "    pred_pos = torch.where(output > threshold,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos) + eps\n",
        "    denominator = true_pos + pred_pos + eps\n",
        "\n",
        "    dice = (numerator.sum())/denominator.sum()\n",
        "    return dice\n",
        "# function - save massive dataset of all predictions for all files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpZ-Ufq9jeVC",
        "colab_type": "code",
        "outputId": "15ed3a85-5d5f-4ebb-8020-af549cdfb606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "#x,y,mode,name = dataset.get_3d_array(0)\n",
        "print(\"Loaded)\")\n",
        "#result = segment(model,checkpoint_dict,x,device)\n",
        "print(\"Segmented\")\n",
        "#output = dt_refine(result,tree)\n",
        "output = expand_dim(output)\n",
        "print(\"Refined\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded)\n",
            "Segmented\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-212299cc4a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Segmented\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#output = dt_refine(result,tree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpand_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Refined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-fc1fe5cad90a>\u001b[0m in \u001b[0;36mexpand_dim\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mexpanded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexpanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6qd0sK_FJER",
        "colab_type": "code",
        "outputId": "5203a7c8-dfaa-499e-da04-2eb78104d835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 512, 147])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16-KZ-pPk63A",
        "colab_type": "code",
        "outputId": "61b17629-91f2-4fe6-ac71-a0d58e1ff8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "per_class,overall = Dice_3D(result,y)\n",
        "print(\"Dice1\",per_class)\n",
        "per_class_dt,overall_dt = Dice_3D_alt(output,y)\n",
        "print(\"Dice2\",per_class_dt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dice1 tensor([0.9806, 0.7609, 0.0040, 0.9221, 0.3274, 0.8180, 0.8819, 0.2099, 0.8899,\n",
            "        0.7918, 0.6731, 0.2842, 0.7526, 0.5014])\n",
            "Dice2 tensor([0.9904, 0.9099, 0.7071, 0.8761, 0.2568, 0.7187, 0.9262, 0.6149, 0.8368,\n",
            "        0.7466, 0.5578, 0.3721, 0.4669, 0.4185])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHyghKPtLFIY",
        "colab_type": "code",
        "outputId": "90fb9410-9c86-4ec5-9067-6e123e7d7270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0522)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}