{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate Final Results",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerekGloudemans/segmentation-medical-images/blob/master/Generate_Final_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TCkBvB9NxZP",
        "colab_type": "code",
        "outputId": "13cba093-6774-47e7-922a-88d70c7aa8ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wltslcNiN4yq",
        "colab_type": "code",
        "outputId": "cda22af1-3a97-4952-aaef-1f58c7124c4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#%%capture \n",
        "!pip install -q --upgrade ipython==5.5.0\n",
        "!pip install -q --upgrade ipykernel==4.6.0\n",
        "!pip3 install torchvision\n",
        "!pip3 install opencv-python\n",
        "\n",
        "import ipywidgets\n",
        "import traitlets\n",
        "# imports\n",
        "\n",
        "# this seems to be a popular thing to do so I've done it here\n",
        "#from __future__ import print_function, division\n",
        "\n",
        "\n",
        "# torch and specific torch packages for convenience\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils import data\n",
        "from torch import multiprocessing\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# for convenient data loading, image representation and dataset management\n",
        "from torchvision import models, transforms\n",
        "import torchvision.transforms.functional as FT\n",
        "from PIL import Image, ImageFile, ImageStat\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from scipy.ndimage import affine_transform\n",
        "import cv2\n",
        "\n",
        "# always good to have\n",
        "import time\n",
        "import os\n",
        "import numpy as np    \n",
        "import _pickle as pickle\n",
        "import random\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import nibabel as nib\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▏                            | 10kB 36.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 81kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnj9qr3g4Smm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def track_segment(data,pred,model):\n",
        "  \"Note that pred must be integer, 0 or 1\"\n",
        "\n",
        "  num_slices = data.shape[2]\n",
        "  result = torch.zeros(data.shape)\n",
        "\n",
        "  for idx in range(num_slices):\n",
        "    # resize to 256 x 256\n",
        "    slice = data[:,:,idx]\n",
        "    \n",
        "    # get slice of \n",
        "    if idx == 0:\n",
        "      prev_slice = torch.zeros(slice.shape)\n",
        "    else:\n",
        "      prev_slice = pred[:,:,idx-1]\n",
        "\n",
        "    original_shape = slice.shape\n",
        "\n",
        "    slice =  Image.fromarray(slice.data.numpy()).copy()\n",
        "    slice = FT.to_grayscale(slice)\n",
        "    slice = FT.to_tensor(slice)\n",
        "    slice = slice.unsqueeze(0)\n",
        "    prev_slice = prev_slice.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    slice = F.interpolate(slice,size = [256,256],mode = 'bilinear')\n",
        "    prev_slice = F.interpolate(prev_slice, size = [256,256],mode = 'nearest')\n",
        "\n",
        "    x = torch.cat((prev_slice,slice),dim = 1)\n",
        "    x = x.to(device).float()\n",
        "\n",
        "    out_slice = model(x)\n",
        "    out_slice = F.interpolate(out_slice,original_shape,mode = 'nearest')\n",
        "    result[:,:,idx] = out_slice.data.cpu()\n",
        "\n",
        "    if False:\n",
        "      plt.figure(figsize = (5,15))\n",
        "      plt.subplot(132)\n",
        "      plt.imshow(prev_slice[0][0],cmap = \"gray\")\n",
        "      plt.clim(0,1)\n",
        "      plt.subplot(131)\n",
        "      plt.imshow(slice[0][0],cmap = \"gray\")\n",
        "      plt.clim(0,1)\n",
        "      plt.subplot(133)\n",
        "      plt.imshow(result[:,:,idx],cmap = \"gray\")\n",
        "      plt.clim(0,1)\n",
        "      plt.show()\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HSeCWeeN6xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        n_classes=1,\n",
        "        depth=3,\n",
        "        wf=4,\n",
        "        padding=True,\n",
        "        batch_norm=False,\n",
        "        up_mode='upconv',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(\n",
        "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "        innum = 1000\n",
        "        midnum = 100\n",
        "        outnum = 4*n_classes\n",
        "        self.reg = nn.Sequential(\n",
        "            nn.BatchNorm1d(innum),\n",
        "            nn.Linear(innum,midnum),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(midnum,outnum),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #for param in self.parameters():\n",
        "        #  param.requires_grad = True\n",
        "\n",
        "    def forward(self, x,BBOX = False):\n",
        "        blocks = []\n",
        "        \n",
        "        # encoder\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path) - 1:\n",
        "                blocks.append(x)\n",
        "                x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # do bbox regression here\n",
        "        if BBOX:\n",
        "          x_reg = x.view(-1)\n",
        "          x_reg = self.reg(x_reg)\n",
        "          bboxes = x_reg.view(4,-1)\n",
        "\n",
        "        # decoder\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "\n",
        "        #CHANGE THIS LINE FOR MULTIPLE OUTPUT CHANNELS\n",
        "        # apply per_class last layer and per-class Softmax \n",
        "        #x = nn.Softmax2d(self.last(x)) \n",
        "        x = torch.sigmoid(self.last(x))\n",
        "        \n",
        "        if BBOX:\n",
        "          return x, bboxes\n",
        "        else:\n",
        "          return x\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
        "            )\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[\n",
        "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNet2(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=2,\n",
        "        n_classes=1,\n",
        "        depth=3,\n",
        "        wf=4,\n",
        "        padding=True,\n",
        "        batch_norm=False,\n",
        "        up_mode='upconv',\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of\n",
        "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "        (Ronneberger et al., 2015)\n",
        "        https://arxiv.org/abs/1505.04597\n",
        "        Using the default arguments will yield the exact version used\n",
        "        in the original paper\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            n_classes (int): number of output channels\n",
        "            depth (int): depth of the network\n",
        "            wf (int): number of filters in the first layer is 2**wf\n",
        "            padding (bool): if True, apply padding such that the input shape\n",
        "                            is the same as the output.\n",
        "                            This may introduce artifacts\n",
        "            batch_norm (bool): Use BatchNorm after layers with an\n",
        "                               activation function\n",
        "            up_mode (str): one of 'upconv' or 'upsample'.\n",
        "                           'upconv' will use transposed convolutions for\n",
        "                           learned upsampling.\n",
        "                           'upsample' will use bilinear upsampling.\n",
        "        \"\"\"\n",
        "        super(UNet2, self).__init__()\n",
        "        assert up_mode in ('upconv', 'upsample')\n",
        "        self.padding = padding\n",
        "        self.depth = depth\n",
        "        prev_channels = in_channels\n",
        "        self.down_path = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.down_path.append(\n",
        "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.up_path = nn.ModuleList()\n",
        "        for i in reversed(range(depth - 1)):\n",
        "            self.up_path.append(\n",
        "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
        "            )\n",
        "            prev_channels = 2 ** (wf + i)\n",
        "\n",
        "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
        "\n",
        "        innum = 1000\n",
        "        midnum = 100\n",
        "        outnum = 4*n_classes\n",
        "        self.reg = nn.Sequential(\n",
        "            nn.BatchNorm1d(innum),\n",
        "            nn.Linear(innum,midnum),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(midnum,outnum),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        #for param in self.parameters():\n",
        "        #  param.requires_grad = True\n",
        "\n",
        "    def forward(self, x,BBOX = False):\n",
        "        blocks = []\n",
        "        \n",
        "        # encoder\n",
        "        for i, down in enumerate(self.down_path):\n",
        "            x = down(x)\n",
        "            if i != len(self.down_path) - 1:\n",
        "                blocks.append(x)\n",
        "                x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # do bbox regression here\n",
        "        if BBOX:\n",
        "          x_reg = x.view(-1)\n",
        "          x_reg = self.reg(x_reg)\n",
        "          bboxes = x_reg.view(4,-1)\n",
        "\n",
        "        # decoder\n",
        "        for i, up in enumerate(self.up_path):\n",
        "            x = up(x, blocks[-i - 1])\n",
        "\n",
        "        #CHANGE THIS LINE FOR MULTIPLE OUTPUT CHANNELS\n",
        "        # apply per_class last layer and per-class Softmax \n",
        "        #x = nn.Softmax2d(self.last(x)) \n",
        "        x = torch.sigmoid(self.last(x))\n",
        "        \n",
        "        if BBOX:\n",
        "          return x, bboxes\n",
        "        else:\n",
        "          return x\n",
        "\n",
        "\n",
        "class UNetConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
        "        super(UNetConvBlock, self).__init__()\n",
        "        block = []\n",
        "\n",
        "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
        "        block.append(nn.ReLU())\n",
        "        if batch_norm:\n",
        "            block.append(nn.BatchNorm2d(out_size))\n",
        "\n",
        "        self.block = nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class UNetUpBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
        "        super(UNetUpBlock, self).__init__()\n",
        "        if up_mode == 'upconv':\n",
        "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
        "        elif up_mode == 'upsample':\n",
        "            self.up = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
        "            )\n",
        "\n",
        "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
        "\n",
        "    def center_crop(self, layer, target_size):\n",
        "        _, _, layer_height, layer_width = layer.size()\n",
        "        diff_y = (layer_height - target_size[0]) // 2\n",
        "        diff_x = (layer_width - target_size[1]) // 2\n",
        "        return layer[\n",
        "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, bridge):\n",
        "        up = self.up(x)\n",
        "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
        "        out = torch.cat([up, crop1], 1)\n",
        "        out = self.conv_block(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Nifti_Dataset(data.Dataset):\n",
        "  def __init__(self,mode = \"view\",dim = 2,class_id = 1):\n",
        "    \"\"\"\n",
        "    Save the last 0.15 proportion of files after sorting for use as validation set.\n",
        "    Loads each slice of the input images as a separate image\n",
        "    mode - view,train or val, defined in same dataset to maintain data separation\n",
        "      view - performs transforms but does not normalize images\n",
        "      train - normalizes data and performs transforms\n",
        "      val - normalizes data, no augmenting transforms\n",
        "    dim - specifies dimension along which to slice image\n",
        "    \"\"\"\n",
        "\n",
        "    self.mode = mode\n",
        "    self.dim = dim\n",
        "    self.class_id = class_id\n",
        "  \n",
        "    data_dir = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Training/img\"\n",
        "    label_dir = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Training/label\"\n",
        "\n",
        "    # get all data and label file names\n",
        "    self.data_files = []\n",
        "    for file in os.listdir(data_dir):\n",
        "      self.data_files.append(os.path.join(data_dir,file))\n",
        "    self.data_files.sort()\n",
        "\n",
        "    self.label_files = []\n",
        "    for file in os.listdir(label_dir):\n",
        "      self.label_files.append(os.path.join(label_dir,file))\n",
        "    self.label_files.sort()\n",
        "\n",
        "    # for each data_file\n",
        "    self.train_data = []\n",
        "    self.val_data = []\n",
        "\n",
        "    for i in range(len(self.data_files)):\n",
        "      data = nib.load(self.data_files[i])\n",
        "      data = np.array(data.get_fdata())\n",
        "\n",
        "      label = nib.load(self.label_files[i])\n",
        "      label = np.array(label.get_fdata()).astype(float)\n",
        "\n",
        "      identifier = self.data_files[i].split(\"_\")[0]\n",
        "      for slice in range(0,data.shape[dim]):\n",
        "\n",
        "        # get slices\n",
        "        if dim == 0:\n",
        "          data_slice = data[slice,:,:]\n",
        "          label_slice = label[slice,:,:]\n",
        "        elif dim == 1:\n",
        "          data_slice = data[:,slice,:]\n",
        "          label_slice = label[:,slice,:]\n",
        "        elif dim == 2:\n",
        "          data_slice = data[:,:,slice]\n",
        "          label_slice = label[:,:,slice]\n",
        "\n",
        "        mean,std = np.mean(data_slice),np.std(data_slice)\n",
        "        # define item dict to store info\n",
        "        item = {\n",
        "            \"identifier\":identifier,\n",
        "            \"slice\":slice,\n",
        "            \"data\":data_slice,\n",
        "            \"label\":label_slice,\n",
        "            \"mean\":mean,\n",
        "            \"std\":std\n",
        "            }\n",
        "\n",
        "        # check to make sure this example actually has organs in it\n",
        "        test = np.bincount(label_slice.astype(int).reshape(-1))\n",
        "        if len(test) == 1: \n",
        "          continue\n",
        "\n",
        "        # make sure 50% of examples have organ of interest in them\n",
        "        if class_id not in np.unique(label_slice.astype(int).reshape(-1)):\n",
        "            if np.random.rand() > 0.5:\n",
        "              continue\n",
        "\n",
        "        # assign to either training or validation data\n",
        "        if i < len(self.data_files) * 0.85:\n",
        "          self.train_data.append(item)\n",
        "        else:\n",
        "          self.val_data.append(item)\n",
        "\n",
        "      #break # to shorten loading time\n",
        "\n",
        "    # define some transforms for training dataset\n",
        "    self.train_transforms = transforms.Compose([\n",
        "          transforms.ColorJitter(brightness = 0.2,contrast = 0.2,saturation = 0.1),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.RandomErasing(p=0.0015, scale=(0.4, 0.6), ratio=(0.3, 3.3), value=0, inplace=False), # big\n",
        "          transforms.RandomErasing(p=0.003, scale=(0.1, 0.3), ratio=(0.3, 3.3), value=0, inplace=False), # medium\n",
        "          transforms.RandomErasing(p=0.004, scale=(0.05, 0.15), ratio=(0.3, 3.3), value=0, inplace=False),# small\n",
        "          transforms.RandomErasing(p=0.0035, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=0, inplace=False) # small\n",
        "          \n",
        "\n",
        "        ])\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      #get relevant dictionary from self memory\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        item = self.train_data[index]\n",
        "      else:\n",
        "        item = self.val_data[index]\n",
        "\n",
        "      x = Image.fromarray(item['data']).copy()\n",
        "      y = Image.fromarray(item['label']).copy()\n",
        "\n",
        "      # to grayscale\n",
        "      x  = FT.to_grayscale(x)\n",
        "      y = FT.to_grayscale(y)\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        # randomly flip and rotate both\n",
        "        FLIP = 0 #np.random.rand()\n",
        "        if FLIP > 0.5:\n",
        "          x = FT.hflip(x)\n",
        "          y = FT.hflip(y)\n",
        "\n",
        "        ROTATE = 0 #np.random.rand()*60 - 30\n",
        "        x  = x.rotate(ROTATE)\n",
        "        y = y.rotate(ROTATE,Image.NEAREST)\n",
        "\n",
        "      # resize to 224 on shorter dimension\n",
        "      x = FT.resize(x, 256)\n",
        "      y = FT.resize(y,256,Image.NEAREST)\n",
        "\n",
        "      if self.mode in ['train','view']:\n",
        "        # randomly jitter color of data and randomly erase data\n",
        "        x = self.train_transforms(x)\n",
        "      # to tensor\n",
        "      try:\n",
        "        x = FT.to_tensor(x)\n",
        "      except:\n",
        "        pass\n",
        "      y = FT.to_tensor(y)\n",
        "            \n",
        "      # normalize and repeat along color dimension if in train or val mode\n",
        "      if self.mode in ['train','val']:\n",
        "        #x = FT.normalize(x,[item['mean']],[item['std']])\n",
        "        #x = x.repeat(3,1,1)\n",
        "        pass\n",
        "\n",
        "      return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.mode in [\"train\",\"view\"]:\n",
        "      return len(self.train_data)\n",
        "    else:\n",
        "      return len(self.val_data)\n",
        "\n",
        "  def show(self,index):\n",
        "    data,label = self[index]\n",
        "    plt.figure()\n",
        "    plt.subplot(121)\n",
        "    data = data.detach()\n",
        "    plt.imshow(data[0],cmap = \"gray\")\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(label[0],cmap = \"gray\")\n",
        "    plt.show()\n",
        "    # convert each tensor to numpy array\n",
        "\n",
        "  def show_slices(self,idx = 0,dim = 0,organ_id = None):\n",
        "    \"\"\"\n",
        "    A nice utility function for plotting all of the slices along a given dimension\n",
        "    idx - indexes all NIfTI images in dataset\n",
        "    dim - indexes dimension of image\n",
        "    organ_id - if not None, all other organs removed from label\n",
        "    \"\"\"\n",
        "    data = nib.load(self.data_files[idx])\n",
        "    label = nib.load(self.label_files[idx])\n",
        "\n",
        "    data = data.get_fdata()\n",
        "    data = np.array(data)\n",
        "    label = label.get_fdata()\n",
        "    label = np.array(label)\n",
        "\n",
        "    for slice in range(0,data.shape[dim]):\n",
        "      if dim == 0:\n",
        "            data_slice = data[slice,:,:]\n",
        "            label_slice = label[slice,:,:]\n",
        "      elif dim == 1:\n",
        "        data_slice = data[:,slice,:]\n",
        "        label_slice = label[:,slice,:]\n",
        "      elif dim == 2:\n",
        "        data_slice = data[:,:,slice]\n",
        "        label_slice = label[:,:,slice]\n",
        "\n",
        "      if organ_id is not None:\n",
        "        # if a specific label is to be looked at, 0 all others\n",
        "        label_slice = 1.0 - np.ceil(np.abs(label_slice.astype(float)-organ_id)/15.0)\n",
        "\n",
        "      print(np.unique(label_slice))\n",
        "      plt.figure()\n",
        "      plt.subplot(121)\n",
        "      plt.imshow(data_slice,cmap = \"gray\")\n",
        "\n",
        "      plt.subplot(122)\n",
        "      plt.imshow(label_slice,cmap = \"gray\")\n",
        "      plt.show()\n",
        "\n",
        "  def len_3d(self):\n",
        "    return len(self.data_files)\n",
        "\n",
        "  def get_3d_array(self,idx):\n",
        "      \"\"\"\n",
        "      Loads a 3D image as a tensor as well as its label, mode, and file name\n",
        "      \"\"\"\n",
        "      assert idx < len(self.data_files) , \"3D image index out of range, there are {} 3D images\".format(len(self.data_files))\n",
        "\n",
        "      # load data and label as tensors\n",
        "      data = nib.load(self.data_files[idx])\n",
        "      label = nib.load(self.label_files[idx])\n",
        "\n",
        "      data = data.get_fdata()\n",
        "      data = torch.from_numpy(np.array(data))\n",
        "      label = label.get_fdata()\n",
        "      label = torch.from_numpy(np.array(label)).int()\n",
        "\n",
        "      # note whether image is training or validation set\n",
        "      if idx < 0.85 * len(self.data_files):\n",
        "        mode = \"train\"\n",
        "      else:\n",
        "        mode = \"val\"\n",
        "      \n",
        "      return data,label,mode,self.data_files[idx]\n",
        "\n",
        "def load_model(checkpoint_file,model,optimizer):\n",
        "  \"\"\"\n",
        "  Reloads a checkpoint, loading the model and optimizer state_dicts and \n",
        "  setting the start epoch\n",
        "  \"\"\"\n",
        "  checkpoint = torch.load(checkpoint_file)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  all_losses = checkpoint['losses']\n",
        "  all_accs = checkpoint['accs']\n",
        "\n",
        "  return model,optimizer,epoch,all_losses,all_accs\n",
        "\n",
        "\n",
        "def dt_refine(orig,tree):\n",
        "  \"\"\"\n",
        "  Applies decision tree ensembling to original outputs\n",
        "  orig = classes x l x w x h\n",
        "  \"\"\"\n",
        "  output = torch.zeros((orig.shape[1],orig.shape[2],orig.shape[3]))\n",
        "\n",
        "  for i in range(0,orig.shape[1]):\n",
        "    for j in range(0,orig.shape[2]):\n",
        "      \n",
        "        inp = np.zeros([orig.shape[3],17])\n",
        "        inp[:,:14] = orig[:,i,j,:].data.numpy().transpose()\n",
        "        inp[:,14] = i\n",
        "        inp[:,15] = j\n",
        "        for k in range(0,orig.shape[3]):\n",
        "          inp[k,16] = k\n",
        "\n",
        "        output[i,j,:] = torch.from_numpy(tree.predict(inp))\n",
        "  \n",
        "  return output\n",
        "\n",
        "\n",
        "def segment(model,data,device,axis = 2,outfile = None):\n",
        "  \"\"\"\n",
        "  Takes in 3D tensor, slices and segments using UNET, and returns result\n",
        "  Note that result will be a [0,1] tensor corresponding to a certain class (whatever model class was used)\n",
        "  if outfile is not none, saves results in  that file \n",
        "  \"\"\"\n",
        "\n",
        "  num_slices = data.shape[axis]\n",
        "  result = torch.zeros(data.shape)\n",
        "\n",
        "  for idx in range(num_slices):\n",
        "    # resize to 256 x 256\n",
        "    if axis == 0:\n",
        "      slice = data[idx,:,:]\n",
        "    elif axis == 1:\n",
        "      slice = data[:,idx,:]\n",
        "    elif axis == 2:\n",
        "      slice = data[:,:,idx]\n",
        "\n",
        "    original_shape = slice.shape\n",
        "\n",
        "    slice =  Image.fromarray(slice.data.numpy()).copy()\n",
        "    slice = FT.to_grayscale(slice)\n",
        "    slice = FT.to_tensor(slice)\n",
        "    slice = slice.unsqueeze(0)\n",
        "\n",
        "    slice = F.interpolate(slice,size = [256,256],mode = 'bilinear')\n",
        "\n",
        "    x = slice.to(device).float()\n",
        "\n",
        "    out_slice = model(x)\n",
        "    out_slice = F.interpolate(out_slice,original_shape,mode = 'bilinear')\n",
        "\n",
        "    if axis == 0:\n",
        "      result[idx,:,:] = out_slice.data.cpu()\n",
        "    elif axis == 1:\n",
        "      result[:,idx,:] = out_slice.data.cpu()\n",
        "    elif axis == 2:\n",
        "      result[:,:,idx] = out_slice.data.cpu()\n",
        "\n",
        "  if outfile:\n",
        "    torch.save(result, outfile)\n",
        "\n",
        "  return result\n",
        "\n",
        "#def segment(model,checkpoint_dict,data,device,outfile = None):\n",
        "#  \"\"\"\n",
        "#  Takes a 3D tensor and segments it with a series of models, predicting maximum class for each\n",
        "#  \"\"\"\n",
        "#  result = torch.zeros((data.shape[0],data.shape[1],data.shape[2]))\n",
        "#  result = predict_by_slices(model,data,device,outfile = None)\n",
        "#  return result\n",
        "\n",
        "\n",
        "def dice_3D(output,target,threshold= 0.5,eps=1e-07):\n",
        "    ones = torch.ones(target.shape)\n",
        "    zeros = torch.zeros(target.shape)\n",
        "    true_pos = torch.where(target == 1,ones,zeros)\n",
        "    pred_pos = torch.where(output > threshold,ones,zeros)\n",
        "\n",
        "    numerator = 2.0 * torch.mul(true_pos,pred_pos) + eps\n",
        "    denominator = true_pos + pred_pos + eps\n",
        "\n",
        "    dice = (numerator.sum())/denominator.sum()\n",
        "    return dice\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRvV2jPsAA3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def thresh_plot(data,step = 0.02):\n",
        "  plt.figure(figsize = (10,15))\n",
        "  from matplotlib import style\n",
        "  style.use(\"fivethirtyeight\")\n",
        "\n",
        "  x = np.arange(0,1,step)\n",
        "  legend = []\n",
        "  for key in data:\n",
        "    series = data[key]\n",
        "    plt.plot(x,series)\n",
        "    legend.append(key)\n",
        "  \n",
        "  plt.xlabel(\"Discrimination Threshold\")\n",
        "  plt.ylabel(\"Dice Accuracy\")\n",
        "  plt.xlim([0,1])\n",
        "  plt.ylim([0,1])\n",
        "  plt.title(\"Discrimination Threshold Accuracy for Various Pooling Sizes\")\n",
        "  plt.legend(legend)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yD_utsqELFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reachability_cluster_mask(input):\n",
        "  \"\"\"\n",
        "  Uses reachability to cluster a 3D tensor of 0,1\n",
        "  \"\"\"\n",
        "  data = input.clone()\n",
        "  cluster_val = 2\n",
        "  cluster_sizes = {}\n",
        "\n",
        "  ones = torch.ones(input.shape)\n",
        "  zeros = torch.zeros(input.shape)\n",
        "  original_length = len(torch.where(data == 1, ones, zeros).nonzero())\n",
        "  queue = []\n",
        "  contin = True\n",
        "  while True:\n",
        "    if cluster_val > 1000:\n",
        "      break\n",
        "    cluster_sizes[cluster_val] = 0\n",
        "    mask = torch.where(data==1,ones,zeros)\n",
        "    indices = mask.nonzero()\n",
        "\n",
        "    # if any values in tensor are still 1 continue (unclustered)\n",
        "    if len(indices) > 1:\n",
        "      first = indices[0]\n",
        "      queue.append(first)\n",
        "    else:\n",
        "      contin = False\n",
        "      break\n",
        "\n",
        "    #print(\"Assigning cluster {}\".format(cluster_val))\n",
        "\n",
        "    while len(queue) > 0:\n",
        "      cur = queue.pop(0)\n",
        "      val = data[cur[0],cur[1],cur[2]]\n",
        "      if val == 1:\n",
        "        cluster_sizes[cluster_val] += 1\n",
        "        data[cur[0],cur[1],cur[2]] = cluster_val # assign to cluster\n",
        "        # add neighbors to queue\n",
        "        add_indices = [[cur[0],cur[1],cur[2]+1],\n",
        "                        [cur[0],cur[1],cur[2]-1],\n",
        "                        [cur[0],cur[1]+1,cur[2]],\n",
        "                        [cur[0],cur[1]-1,cur[2]],\n",
        "                        [cur[0]-1,cur[1],cur[2]],\n",
        "                        [cur[0]+1,cur[1],cur[2]]]\n",
        "        if cur[0] < data.shape[0]-1 and cur[1] < data.shape[1]-1 and cur[2] < data.shape[2] - 1:\n",
        "          for item in add_indices:\n",
        "            queue.append(item)\n",
        "\n",
        "    cluster_val += 1\n",
        "    \n",
        "  biggest_idx = 2\n",
        "  biggest_size = 0\n",
        "  for key in cluster_sizes:\n",
        "    if cluster_sizes[key] > biggest_size:\n",
        "      biggest_idx = key\n",
        "      biggest_size = cluster_sizes[key]\n",
        "  # get biggest cluster\n",
        "  mask1 = torch.where((data == biggest_idx), ones, 1.0 - ones)\n",
        "  mask2 = torch.where((data == 0), ones, 1.0 - ones)\n",
        "  mask = mask1+mask2\n",
        "  return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5D-xUJFAEmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(volume,model,device):\n",
        "  checkpoint = torch.load(checkpoint_dict[\"dim0\"])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  baseline0 = segment(model,volume,device,axis = 0)\n",
        "\n",
        "  checkpoint = torch.load(checkpoint_dict[\"dim1\"])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  baseline1 = segment(model,volume,device,axis = 1)\n",
        "\n",
        "  checkpoint = torch.load(checkpoint_dict[\"dim2\"])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  baseline2 = segment(model,volume,device,axis = 2)\n",
        "\n",
        "  # combine results and avgpool with kernel size 3, and threshold at 0.4\n",
        "  combo = 0.4*baseline0 + 0.2* baseline1 + 0.4* baseline2\n",
        "  p=3\n",
        "  avg_combo = F.avg_pool3d(combo.unsqueeze(0),p,stride = 1,padding = int((p-1)/2)).squeeze(0)\n",
        "  ones = torch.ones(avg_combo.shape)\n",
        "  avg_combo = torch.where(avg_combo > 0.4,ones, 1.0-ones)\n",
        "\n",
        "  # remove clusters from combination\n",
        "  mask = reachability_cluster_mask(avg_combo)\n",
        "  declustered = torch.mul(mask,combo)\n",
        "\n",
        "  # apply final average pooling with threshold ___\n",
        "  final = F.avg_pool3d(combo.unsqueeze(0),p,stride = 1,padding = int((p-1)/2)).squeeze(0)\n",
        "  final_thresholded = torch.where(final > 0.4,ones, 1.0-ones)\n",
        "\n",
        "  return final_thresholded\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpUDyDF52LnO",
        "colab_type": "text"
      },
      "source": [
        "# Load everything up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIw8X9XLQ5DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dict = {\n",
        "    \"dim0\":\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/organ1_dim0_e22.pt\",\n",
        "    \"dim1\":\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/organ1_dim1_e15.pt\",\n",
        "    \"dim2\":\"/content/drive/My Drive/Colab Notebooks/Segmentation/checkpoints/Keepers/Final/organ1_dim2_e120.pt\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHe25a0mO5Mg",
        "colab_type": "code",
        "outputId": "99d26c7e-eb03-450b-f37a-35d372efa44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.cuda.empty_cache()   \n",
        "\n",
        "model = UNet()\n",
        "print (\"Model loaded.\")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "checkpoint = torch.load(checkpoint_dict['dim1'])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "try:\n",
        "  dataset\n",
        "except:\n",
        "  dataset = Nifti_Dataset(mode = \"train\",dim = 2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThY1rBo9EJhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "cfff1d17-8328-4a71-c966-279854a12f9f"
      },
      "source": [
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "for idx in range(dataset.len_3d()):\n",
        "  volume,label,mode,name = dataset.get_3d_array(idx)\n",
        "\n",
        "  output = predict(volume,model,device)\n",
        "  dice = dice_3D(output,label)  \n",
        "  if mode == \"train\":\n",
        "    train_accs.append(dice)\n",
        "  elif mode == \"val\":\n",
        "    val_accs.append(dice)\n",
        "  \n",
        "  print(\"{} volume {} dice score: {}\".format(mode,idx,dice))\n",
        "\n",
        "train_acc = sum(train_accs)/len(train_accs)\n",
        "val_acc = sum(val_accs)/len(val_accs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train volume 0 dice score: 0.9473950862884521\n",
            "train volume 1 dice score: 0.92239910364151\n",
            "train volume 2 dice score: 0.8817955255508423\n",
            "train volume 3 dice score: 0.9443104267120361\n",
            "train volume 4 dice score: 0.9367591738700867\n",
            "train volume 5 dice score: 0.8528770208358765\n",
            "train volume 6 dice score: 0.8491819500923157\n",
            "train volume 7 dice score: 0.9404333233833313\n",
            "train volume 8 dice score: 0.9493824243545532\n",
            "train volume 9 dice score: 0.921882152557373\n",
            "train volume 10 dice score: 0.9110795855522156\n",
            "train volume 11 dice score: 0.9425414204597473\n",
            "train volume 12 dice score: 0.9118952751159668\n",
            "train volume 13 dice score: 0.950798511505127\n",
            "train volume 14 dice score: 0.8930566906929016\n",
            "train volume 15 dice score: 0.9424028396606445\n",
            "train volume 16 dice score: 0.9366868734359741\n",
            "train volume 17 dice score: 0.9535695314407349\n",
            "train volume 18 dice score: 0.9387070536613464\n",
            "train volume 19 dice score: 0.9370836019515991\n",
            "train volume 20 dice score: 0.9355854988098145\n",
            "train volume 21 dice score: 0.9350882768630981\n",
            "train volume 22 dice score: 0.9374732375144958\n",
            "train volume 23 dice score: 0.9638603925704956\n",
            "train volume 24 dice score: 0.9573974609375\n",
            "train volume 25 dice score: 0.7190314531326294\n",
            "val volume 26 dice score: 0.8786771893501282\n",
            "val volume 27 dice score: 0.8215875625610352\n",
            "val volume 28 dice score: 0.958614706993103\n",
            "val volume 29 dice score: 0.9443533420562744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkZk58ELIO-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a6e5f0c2-3e16-4c3b-fd91-393ad16f13c6"
      },
      "source": [
        "train = np.array(train_accs)\n",
        "print(np.mean(train))\n",
        "print(np.std(train))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9197182\n",
            "0.049323957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3nP9nqJIi-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "34b83086-aa56-4770-d7fe-679c96831beb"
      },
      "source": [
        "val = np.array(val_accs)\n",
        "print(np.mean(val))\n",
        "print(np.std(val))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.90080816\n",
            "0.05478016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R52d3SWxPxcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "9e7481f2-b89d-4c23-914d-3f18aecf26c8"
      },
      "source": [
        "# generate test volumes\n",
        "test_directory = \"/content/drive/My Drive/Colab Notebooks/Segmentation/RawData/Testing/img\"\n",
        "\n",
        "for file in os.listdir(test_directory):\n",
        "  full_file = os.path.join(test_directory,file)\n",
        "\n",
        "  data = nib.load(full_file)\n",
        "  outname = \"predictions_\" + file\n",
        "  data = data.get_fdata()\n",
        "  volume = torch.from_numpy(np.array(data))\n",
        "  \n",
        "  output = predict(volume,model,device).data.numpy()\n",
        "  out_file = nib.Nifti1Image(output,affine=np.eye(4))\n",
        "  nib.save(out_file, os.path.join(test_directory,outname))\n",
        "  print(\"Wrote output file {}\".format(outname))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wrote output file predictions_img0061.nii.gz\n",
            "Wrote output file predictions_img0062.nii.gz\n",
            "Wrote output file predictions_img0063.nii.gz\n",
            "Wrote output file predictions_img0064.nii.gz\n",
            "Wrote output file predictions_img0065.nii.gz\n",
            "Wrote output file predictions_img0066.nii.gz\n",
            "Wrote output file predictions_img0067.nii.gz\n",
            "Wrote output file predictions_img0068.nii.gz\n",
            "Wrote output file predictions_img0069.nii.gz\n",
            "Wrote output file predictions_img0070.nii.gz\n",
            "Wrote output file predictions_img0071.nii.gz\n",
            "Wrote output file predictions_img0072.nii.gz\n",
            "Wrote output file predictions_img0073.nii.gz\n",
            "Wrote output file predictions_img0074.nii.gz\n",
            "Wrote output file predictions_img0075.nii.gz\n",
            "Wrote output file predictions_img0076.nii.gz\n",
            "Wrote output file predictions_img0077.nii.gz\n",
            "Wrote output file predictions_img0078.nii.gz\n",
            "Wrote output file predictions_img0079.nii.gz\n",
            "Wrote output file predictions_img0080.nii.gz\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}